{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(480, 640)\n",
      "uint8\n",
      "height: 480, width: 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 19:44:49.814 python[65193:8407963] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-20 19:44:49.815 python[65193:8407963] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 # opencv-python which is an Open Source Computer Vision Library\n",
    "import sys\n",
    "import matplotlib.pyplot as plt # matplotlib.pyplot which provides a MATLAB-like plotting framework\n",
    "\n",
    "def close_window():\n",
    "    cv2.destroyAllWindows()  # Close the window\n",
    "    cv2.waitKey(1)\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "def ifNotExistExit(filename):\n",
    "    if not filename:\n",
    "        print('filename is None')\n",
    "        sys.exit()\n",
    "\n",
    "img = cv2.imread('./fig/dog.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if img is None:\n",
    "\tifNotExistExit(img)\n",
    "\n",
    "print(type(img)) # class numpy\n",
    "print(img.shape) # bgr (rgb x)\n",
    "print(img.dtype)\n",
    "\n",
    "h, w = img.shape\n",
    "print(f'height: {h}, width: {w}')\n",
    "\n",
    "cv2.namedWindow('img', cv2.WINDOW_AUTOSIZE) # create a window with the name 'img' and set the window size to fit the image\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "while True:\n",
    "    if cv2.waitKey(1000) & 0xFF == ord('q'):  # Press 'q' to close the window\n",
    "        break\n",
    "\n",
    "cv2.waitKey() # wait for any key to close the window \n",
    "close_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## url image usage\n",
    "from urllib import request\n",
    "\n",
    "url = 'https://www.google.com/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png'\n",
    "req = request.urlopen(url)\n",
    "arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
    "img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "cv2.imwrite('./fig/google.png', img)\n",
    "\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey()\n",
    "close_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = np.ones((200, 200, 3), np.uint8) * 255   # whtie canvas\n",
    "img2 = np.zeros((200, 200, 3), np.uint8)        # black canvas\n",
    "\n",
    "cv2.imshow(\"White Canvas\", img1)\n",
    "cv2.imshow(\"Black Canvas\", img2)\n",
    "\n",
    "cv2.waitKey()\n",
    "close_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## draw line on the image on the canvas\n",
    "\n",
    "white_bg_img = np.ones((600, 1200, 3), np.uint8) * 255 # white canvas\n",
    "\n",
    "# draw a red line on the white canvas\n",
    "cv2.line(white_bg_img, (50, 100), (300, 100), (0, 0, 255), 3, cv2.LINE_AA)\n",
    "# draw a green line on the white canvas\n",
    "cv2.line(white_bg_img, (300, 100), (250, 400), (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "# draw a blue arrowed line on the white canvas\n",
    "cv2.arrowedLine(white_bg_img, (250, 400), (400, 400), (255, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "# draw a yellow rectangle on the white canvas\n",
    "cv2.rectangle(white_bg_img, (500, 100), (800, 400), (0, 255, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "# draw a cyan filled rectangle on the white canvas\n",
    "cv2.rectangle(white_bg_img, (900, 100), (1100, 400), (255, 255, 0), -1, cv2.LINE_AA)\n",
    "\n",
    "# draw a magenta circle on the white canvas\n",
    "cv2.circle(white_bg_img, (150, 500), 50, (255, 0, 255), 3, cv2.LINE_AA)\n",
    "\n",
    "# draw a green filled circle on the white canvas\n",
    "cv2.circle(white_bg_img, (350, 500), 50, (0, 255, 0), -1, cv2.LINE_AA)\n",
    "\n",
    "# draw a blue ellipse on the white canvas\n",
    "cv2.ellipse(white_bg_img, (600, 500), (100, 50), 0, 0, 360, (255, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "# show the white canvas\n",
    "cv2.imshow('white_bg_img', white_bg_img)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    if cv2.waitKey(1000) & 0xFF == ord('q'):  # Press 'q' to close the window\n",
    "        break\n",
    "\n",
    "cv2.waitKey()  # wait for any key to close the window\n",
    "close_window() # close the window\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Channel which is the transparency of the color is not supported in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "## 알파채널\n",
    "\n",
    "img_hat = cv2.imread('./fig/hat_alpha.png', cv2.IMREAD_UNCHANGED) # IMREAD_UNCHANGED flag to read the image with the alpha channel\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if img_hat is None:\n",
    "    print(\"Error: Image not found or unable to load.\")\n",
    "else:\n",
    "    # Extract the alpha channel and RGB channels\n",
    "    hat_mst = img_hat[:, :, 3]  # alpha channel\n",
    "    hat_rgb = img_hat[:, :, :3]  # rgb channel\n",
    "\n",
    "    # Display the images\n",
    "    cv2.imshow('img_hat', img_hat)\n",
    "    cv2.imshow('hat_mst', hat_mst)\n",
    "    cv2.imshow('hat_rgb', hat_rgb)\n",
    "    cv2.waitKey()\n",
    "    close_window()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든이미지는 Histogram (히스토그램)을 이용하여 그림을 그린다.\n",
    "히스토그램에서 X축과 Y축의 의미는 다음과 같습니다: \n",
    "[ref 2503_KOSA_OpenCV_5d.pdf 13page](2503_KOSA_OpenCV_5d.pdf)\n",
    "\n",
    "축\t의미\n",
    "X축 (가로축)\t***픽셀의 밝기(명암) 값***을 나타냄. 일반적으로 0(완전한 검정)부터 255(완전한 흰색)까지의 그레이스케일 값을 가짐.\n",
    "Y축 (세로축)\t해당 밝기 값(명암 값)을 가진 **픽셀의 개수(빈도)**를 나타냄. 즉, 특정 밝기를 가지는 픽셀이 이미지 내에서 얼마나 많이 존재하는지를 보여줌.\n",
    "\n",
    "\n",
    "해석 방법\n",
    "히스토그램의 왼쪽 영역: 어두운 픽셀(검정에 가까운 부분)이 많을수록 높아짐.\n",
    "히스토그램의 오른쪽 영역: 밝은 픽셀(흰색에 가까운 부분)이 많을수록 높아짐.\n",
    "히스토그램이 균등하게 분포되어 있다면, 이미지의 **명암 대비(contrast)**가 좋은 상태임.\n",
    "특정 부분에 몰려 있다면, 이미지가 너무 어둡거나 밝은 상태일 수 있음.\n",
    "주어진 이미지에서는 왼쪽(어두운 영역)과 오른쪽(밝은 영역)에서 픽셀 수가 많이 나타나므로 명암 대비가 뚜렷한 이미지로 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형상의 화소처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 220)\n",
      "uint8\n",
      "(220, 220)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "## 영상의 화소처리\n",
    "\n",
    "src = cv2.imread('./fig/lenna.png', cv2.IMREAD_GRAYSCALE) # default value is cv2.IMREAD_COLOR\n",
    "src2 = np.zeros_like(src) # create a black image with the same size as the source image\n",
    "\n",
    "print(src.shape) # (height, width, channel)\n",
    "print(src.dtype) # uint8\n",
    "\n",
    "print(src2.shape) # (height, width, channel)\n",
    "print(src2.dtype) # uint8\n",
    "\n",
    "#gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY) # convert the color image to grayscale image\n",
    "\n",
    "dst = cv2.add(src, 50) # add 50 to all pixels, if the pixel value is greater than 255, it will be set to 255 (not over maximum value)\n",
    "dst_add_red = cv2.add(src, (50, 0, 0, 0)) # add 50 to the red channel only\n",
    "dst_add_green = cv2.add(src, (0, 50, 0, 0)) # add 50 to the green channel only\n",
    "dst_add_blue = cv2.add(src, (0, 0, 50, 0)) # add 50 to the blue channel only\n",
    "\n",
    "\n",
    "#dst2 = cv2.subtract(src, 50) # subtract 50 from all pixels, if the pixel value is less than 0, it will be set to 0 (not below minimum value)\n",
    "\n",
    "cv2.circle(src2, (110,110), 80, 200, -1) # draw a filled circle on the black image\n",
    "cv2.circle(src2, (110,110), 40, 50, -1) # draw a filled circle on the black image\n",
    "\n",
    "dst1 = cv2.add(src, src2) # add two images\n",
    "dst2 = cv2.addWeighted(src, 0.5, src2, 0.5, 0) # add two images with the same weight\n",
    "dst3 = cv2.subtract(src, src2) # subtract two images\n",
    "dst4 = cv2.absdiff(src, src2) # get the absolute difference between two images\n",
    "dst5 = cv2.bitwise_and(src, src2) # bitwise AND operation between two images\n",
    "dst6 = cv2.bitwise_or(src, src2) # bitwise OR operation between two images\n",
    "dst7 = cv2.bitwise_xor(src, src2) # bitwise XOR operation between two images\n",
    "dst8 = cv2.bitwise_not(src) # bitwise NOT operation on the source image\n",
    "dst9 = cv2.divide(src, src2) # divide two images\n",
    "dst10 = cv2.multiply(src, src2) # multiply two images\n",
    "\n",
    "cv2.imshow('add', dst1)\n",
    "cv2.imshow('addWeighted', dst2)\n",
    "cv2.imshow('subtract', dst3)\n",
    "cv2.imshow('absdiff', dst4)\n",
    "cv2.imshow('bitwise_and', dst5)\n",
    "cv2.imshow('bitwise_or', dst6)\n",
    "cv2.imshow('bitwise_xor', dst7)\n",
    "cv2.imshow('bitwise_not', dst8)\n",
    "cv2.imshow('divide', dst9)\n",
    "cv2.imshow('multiply', dst10)\n",
    "\n",
    "cv2.imshow('src', src)\n",
    "cv2.imshow('dst', dst)\n",
    "\n",
    "cv2.imshow('dst_add_red', dst_add_red)\n",
    "cv2.imshow('dst_add_green', dst_add_green)\n",
    "cv2.imshow('dst_add_blue', dst_add_blue)\n",
    "\n",
    "cv2.imshow('src2', src2)\n",
    "\n",
    "#cv2.imshow('dst2', dst2)\n",
    "\n",
    "#cv2.imshow('gray', gray)\n",
    "\n",
    "cv2.waitKey()\n",
    "close_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(weight)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m q\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m weight:\n\u001b[1;32m      9\u001b[0m     dst \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39maddWeighted(src, i, background, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mi, \u001b[38;5;241m0.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "## load rose image\n",
    "src = cv2.imread('./fig/rose.jpg', cv2.IMREAD_COLOR) # load the color image\n",
    "background = np.ones_like(src)*255\n",
    "\n",
    "weight = np.arange(0, 1, 0.01)\n",
    "# print(weight)\n",
    "q\n",
    "for i in weight:\n",
    "    dst = cv2.addWeighted(src, i, background, 1-i, 0.0)\n",
    "    cv2.imshow('dst', dst)\n",
    "\n",
    "    if cv2.waitKey(100) == 27:\n",
    "        break\n",
    "\n",
    "cv2.waitKey()\n",
    "close_window()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
