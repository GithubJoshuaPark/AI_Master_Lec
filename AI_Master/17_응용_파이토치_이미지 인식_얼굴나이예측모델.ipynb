{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df175d32",
      "metadata": {},
      "source": [
        "### Chapter 17"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4592499",
      "metadata": {},
      "source": [
        "# **응용_파이토치 이미지 인식 `얼굴로 나이 예측`**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f2c551",
      "metadata": {},
      "source": [
        "> ## 학습목표\n",
        "- 딥러닝 모델을 활용하여 입력된 얼굴 이미지를 정확히 분석하고 개인의 나이를 예측할 수 있도록 프로그래밍 할 수 있다. \n",
        "- 얼굴 이미지의 특징을 효과적으로 추출하기 위해 합성곱 신경망(CNN) 기반의 아키텍처를 설계하고 구현할 수 있다.\n",
        "- 다양한 연령대와 얼굴 조건(조명, 표정, 피부색 등)에 대해 일반화된 예측 성능을 높이기 위해 데이터 증강 및 처리 기법을 적용할 수 있다.\n",
        "- 모델의 학습 결과를 평가하고, 손실 함수 및 하이퍼파라미터 최적화를 통해 예측 정확도를 높일 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21929db7",
      "metadata": {},
      "source": [
        "## **17.1 얼굴 이미지를 입력으로 받아 나이를 예측하는 모델 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "928cf01a",
      "metadata": {},
      "source": [
        "### **UTKFace 데이터 세트는 0~116세의 대규모 얼굴 데이터 세트입니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e130181a",
      "metadata": {},
      "source": [
        "<img src=\"./image/utkface.png\" width=\"800\"/>\n",
        "<figcaption>그림17.1 UTKFace Large Scale Face Dataset (출처 : https://susanqq.github.io/UTKFace/)</figcaption>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b81856a",
      "metadata": {},
      "source": [
        "#### 데이터셋 개요\n",
        "\n",
        "-   20,000개 이상의 다양한 얼굴 이미지 포함\n",
        "-   연령대 : 0세부터 116세까지 폭넓은 분포\n",
        "-   각 이미지에 나이, 성별, 인종 정보가 주석으로 제공\n",
        "\n",
        "#### 이미지 특성\n",
        "\n",
        "-   자연스러운 환경에서 촬영된 단일 얼굴 이미지\n",
        "-   포즈, 표정, 조명, 가림, 해상도 등 다양한 변수 포함\n",
        "-   정렬 및 크롭된 얼굴 이미지 버전 제공\n",
        "-   68개 랜드마크 포인트 정보 포함\n",
        "\n",
        "#### 활용 분야\n",
        "\n",
        "-   얼굴 검출\n",
        "-   나이 추정\n",
        "-   나이 진행/회귀 모델링\n",
        "-   얼굴 랜드마크 위치 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eea0f411",
      "metadata": {},
      "source": [
        "각 얼굴 이미지의 레이블은 파일 이름에 내장되어 있으며 형식은 다음과 같습니다.`[age]_[gender]_[race]_[date&time].jpg`\n",
        "\n",
        "-   `[age]`0~116 사이의 정수로 연령을 나타냅니다.\n",
        "-   `[gender]`0(남자) 또는 1(여자)입니다.\n",
        "-   `[race]`0에서 4까지의 정수이며, 백인, 흑인, 아시아인, 인도인 및 기타(히스패닉계, 라틴계, 중동인 등)를 나타냅니다.\n",
        "-   `[date&time]`yyyymmddHHMMSSFFF 형식으로 이미지가 UTKFace에 수집된 날짜와 시간을 표시합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a567a62e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /Users/joshuapark/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "python(62199) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
            "100%|██████████| 97.8M/97.8M [00:46<00:00, 2.21MB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 106\u001b[0m\n\u001b[1;32m    103\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, criterion, optimizer)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# 학습된 모델 저장\u001b[39;00m\n\u001b[1;32m    109\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trained_model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_estimation_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, _use_new_zipfile_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     75\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    524\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class UTKFaceDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith('.jpg')]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        try:\n",
        "            image = Image.open(img_name).convert('RGB')  # 이미지를 RGB로 변환\n",
        "            age = int(self.image_files[idx].split('_')[0])# 파일명에서 나이 추출\n",
        "            \n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            \n",
        "            return image, age\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "# 데이터 변환 및 정규화 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # ResNet 입력 크기에 맞게 조정\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 통계 사용\n",
        "])\n",
        "\n",
        "# 데이터셋 로드 및 분할\n",
        "dataset = UTKFaceDataset(root_dir='data/UTKface', transform=transform)\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class AgeEstimationModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AgeEstimationModel, self).__init__()\n",
        "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)  \n",
        "        # 사전 학습된 ResNet50 로드\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_ftrs, 1)  \n",
        "        # 출력층을 1개의 노드로 변경 (나이 추정)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        \n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "        \n",
        "        # 검증\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "        \n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        print(f'Validation Loss: {val_loss:.4f}')\n",
        "    \n",
        "    return model\n",
        "\n",
        "# 모델, 손실 함수, 옵티마이저 초기화\n",
        "model = AgeEstimationModel()\n",
        "criterion = nn.MSELoss()  # 평균 제곱 오차 사용\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 모델 학습\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "\n",
        "# 학습된 모델 저장\n",
        "torch.save(trained_model.state_dict(), 'age_estimation_model.pth', _use_new_zipfile_serialization=False)\n",
        "\n",
        "def predict_age(model, image_path):\n",
        "    \"\"\"이미지에서 나이를 예측하는 함수\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "    \n",
        "    predicted_age = output.item()\n",
        "    return round(predicted_age)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0ce6d57",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 모델 로드 및 예측\n",
        "predicted_age = predict_age(model, './data/UTKface/1_0_2_20161219203249692.jpg')\n",
        "print(f'예측된 나이: {predicted_age}세')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c0fd56",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import matplotlib.font_manager as fm\n",
        "from matplotlib import rcParams\n",
        "\n",
        "# 한글 폰트 설정\n",
        "rcParams['font.family'] = 'Malgun Gothic'\n",
        "rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# 폰트 설정 확인\n",
        "print(\"현재 설정된 폰트:\", rcParams['font.family'])\n",
        "\n",
        "# 모델 로드\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.load_state_dict(torch.load('age_estimation_model.pth', map_location=device))\n",
        "\n",
        "def predict_and_display_age(model, image_path):\n",
        "    \"\"\"\n",
        "    이미지의 나이를 예측하고 이미지와 함께 결과를 표시하는 함수\n",
        "    \"\"\"\n",
        "    # 나이 예측\n",
        "    predicted_age = predict_age(model, image_path)\n",
        "    \n",
        "    # 이미지 로드 및 표시\n",
        "    img = Image.open(image_path)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'예측된 나이: {predicted_age}세\\n실제 나이: {image_path.split(\"/\")[-1].split(\"_\")[0]}세')\n",
        "    plt.show()\n",
        "    \n",
        "    return predicted_age\n",
        "\n",
        "# 모델 로드 및 예측\n",
        "image_path = './data/UTKface/36_1_1_20170109141849605.jpg'\n",
        "predicted_age = predict_and_display_age(model, image_path)\n",
        "print(f'예측된 나이: {predicted_age}세')\n",
        "\n",
        "# 80_1_0_20170110140603775\n",
        "# 54_0_0_20170111210520573\n",
        "# 42_0_2_20170104184350086\n",
        "# 36_1_1_20170109141849605\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80ae4115",
      "metadata": {},
      "source": [
        "## **17.2 Korean Face Age Classification**\n",
        "\n",
        "**■ 데이터 소개**\n",
        "\n",
        "총 1,000 가계 (3,000명)이상을 대상으로 80,700장\n",
        "(가족사진 : 6,900, 각도별 개인 사진 : 49,800, 기간별 나이 사진 : 24,000) 이상의 가족 관계 얼굴 이미지 데이터셋을 구축\n",
        "\n",
        "\n",
        "**■ 구축목적**\n",
        "\n",
        "– 가계도 내 가족 관계 정보 데이터셋을 구축함으로써 가족 내 안면 유사도를 추출할 수 있으며, 가족 관계 관련 산업에 활용이 가능함\n",
        "\n",
        "– 총 1,000 가계 이상의 직계 가족 구성원에 대한 안면 데이터를 획득, 정제, 가공하여 경찰청 등의 국가 관리 사업 뿐 아니라, 응용 기술 및 관련 산업 분야의 성장을 도모함 \n",
        "\n",
        "– 가계도 내 인물의 기간별 안면 데이터셋을 구축함으로써 인물 나이 추정이 가능하여 범죄자 나이 예측 등에 다향한 분야로 활용 가능함\n",
        "\n",
        "\n",
        "## **`한국인 얼굴이 주어졌을 때 나이(age)를 예측하는 인공지능`**\n",
        "\n",
        "#### **데이터 세트 소개**\n",
        "\n",
        "* 전체 데이터 세트는 13,068개의 이미지로 구성된다.\n",
        "\n",
        "<pre>\n",
        "F0001_AGE_D_18_a1.jpg\n",
        "F0001_AGE_D_18_a2.jpg\n",
        "...\n",
        "F0900_AGE_M_57_f1.jpg\n",
        "F0900_AGE_M_57_f2.jpg\n",
        "</pre>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea8448b2",
      "metadata": {},
      "source": [
        "\n",
        "<img src=\"image/facedataset.png\" width=\"600px\">\n",
        "\n",
        "\n",
        "* <b>custom_dataset.csv</b>는 각 이미지에 대한 <b>메타 정보</b>를 가진다.\n",
        "  * 속성(attribute) 목록: 'family_id', 'person_id', 'age_class', 'image_path'\n",
        "\n",
        "* 전체 이미지를 학습(training), 검증(validation), 테스트(test) 목적으로 나눈다.\n",
        "  * 학습 데이터 세트: (F0001 ~ F0299) folders have 10,025 images.\n",
        "  * 검증 데이터 세트: (F0801 ~ F0850) folders have 1,539 images.\n",
        "  * 테스트 데이터 세트: (F0851 ~ F0900) folders have 1,504 images.\n",
        "\n",
        "* 본 저장소에서는 각 <b>이미지</b>와 <b>나이 정보(age class)</b>만 사용한다.\n",
        "  * 주어진 한 장의 한국인 얼굴 이미지가 어떠한 나이군에 해당하는지 예측하는 작업을 수행한다.\n",
        "\n",
        "* <b>데이터 세트 다운로드</b>: \n",
        "  * 본 데이터 세트는 [AI Hub \"가족 관계가 알려진 얼굴 이미지 데이터 세트\"](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=528)를 활용한다.\n",
        "\n",
        "  \n",
        "  <img width=\"800\" height=\"\" src=\"./image/family.png\" >\n",
        "\n",
        "#### 최종 테스트 결과\n",
        "<img src=\"image/faceresult.png\" width=\"200px\">\n",
        "\n",
        "출처 : https://github.com/ndb796/korean_face_age_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dcb79f2",
      "metadata": {},
      "source": [
        "## **17.3 Korean Face Age Classification 실습**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d860f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25d860f9",
        "outputId": "066b8728-602e-4a3c-b4a9-ca7c5681f145"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ndb796/korean_face_age_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b27571",
      "metadata": {
        "id": "25b27571"
      },
      "source": [
        "### 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd831e7",
      "metadata": {
        "id": "ecd831e7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a62a80e2",
      "metadata": {
        "id": "a62a80e2"
      },
      "source": [
        "### 데이터 세트 불러오기\n",
        "- Reference: https://github.com/ndb796/korean_face_age_classification "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6082c60e",
      "metadata": {
        "id": "6082c60e"
      },
      "source": [
        "* 데이터 세트를 확인한다.\n",
        "  * <b>Training dataset</b>: (F0001 ~ F0299) folders have 10,025 images.\n",
        "  * <b>Validation dataset</b>: (F0801 ~ F0850) folders have 1,539 images.\n",
        "  * <b>Test dataset</b>: (F0851 ~ F0900) folders have 1,504 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7393124f",
      "metadata": {
        "id": "7393124f"
      },
      "outputs": [],
      "source": [
        "def parsing(meta_data):     \n",
        "    # meta_data를 처리하여 이미지와 나이 클래스에 대한 정보를 추출하는 함수\n",
        "    image_age_list = []     # 이미지 경로와 나이 클래스를 저장할 빈 리스트 생성\n",
        "    # 메타데이터 파일의 모든 행을 순차적으로 처리\n",
        "    for idx, row in meta_data.iterrows():\n",
        "        image_path = row['image_path']   # 현재 행에서 이미지 경로 가져오기\n",
        "        age_class = row['age_class']     # 현재 행에서 나이 클래스 가져오기\n",
        "        image_age_list.append([image_path, age_class])   \n",
        "        # 이미지 경로와 나이 클래스를 리스트에 추가\n",
        "    return image_age_list    # 결과 리스트 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "236c3cf5",
      "metadata": {},
      "source": [
        "PyTorch의 Dataset 클래스를 상속받은 사용자 정의 데이터셋 클래스를 구현한 것입니다.\n",
        "나이 클래스 레이블을 다루는 데이터셋을 처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbbea37",
      "metadata": {
        "id": "3fbbea37"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, meta_data, image_directory, transform=None):\n",
        "        self.meta_data = meta_data  # 메타 데이터 (각 이미지에 대한 정보)\n",
        "        self.image_directory = image_directory  # 이미지가 저장된 디렉토리 경로\n",
        "        self.transform = transform  # 이미지 변환 (예: 데이터 증강 등)\n",
        "        \n",
        "# 메타 데이터를 처리하는 함수 (meta_data에서 나이 클래스와 이미지 경로를 추출)\n",
        "        image_age_list = parsing(meta_data)\n",
        "        self.image_age_list = image_age_list \n",
        "        # 나이 클래스와 이미지 경로를 담고 있는 리스트\n",
        "\n",
        "        # 나이 클래스에서 레이블로 변환하는 매핑\n",
        "        self.age_class_to_label = {\n",
        "            \"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7\n",
        "        }\n",
        "\n",
        "    def __len__(self):   #__len__ 메서드는 데이터셋의 크기를 반환\n",
        "        return len(self.meta_data)   \n",
        "    # meta_data의 길이를 반환하여 데이터셋의 크기를 알 수 있음\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        image_path, age_class = self.image_age_list[idx]   \n",
        "        # 해당 인덱스의 이미지 경로와 나이 클래스 추출\n",
        "        img = Image.open(os.path.join(self.image_directory, image_path))   \n",
        "        # 이미지 파일을 열기\n",
        "        label = self.age_class_to_label[age_class]   \n",
        "        # 나이 클래스를 레이블로 변환\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)  # 변환 함수가 지정되면 이미지를 변환\n",
        "    \n",
        "        return img, label  # 이미지와 레이블 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b63765",
      "metadata": {},
      "source": [
        "__getitem__ 메서드는 데이터셋에서 특정 인덱스에 해당하는 이미지와 레이블을 반환합니다.\n",
        "image_age_list[idx]를 통해 해당 인덱스에 있는 이미지 파일 경로와 나이 클래스(문자)를 얻습니다.\n",
        "Image.open을 사용해 이미지를 열고, age_class_to_label 딕셔너리를 사용하여 나이 클래스 문자열을 숫자 레이블로 변환합니다.\n",
        "만약 transform이 제공되었다면 이미지를 변환합니다.\n",
        "마지막으로 이미지와 해당 레이블을 튜플로 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ec8a03",
      "metadata": {
        "id": "11ec8a03"
      },
      "outputs": [],
      "source": [
        "label_to_age = {\n",
        "    0: \"0-6 years old\",\n",
        "    1: \"7-12 years old\",\n",
        "    2: \"13-19 years old\",\n",
        "    3: \"20-30 years old\",\n",
        "    4: \"31-45 years old\",\n",
        "    5: \"46-55 years old\",\n",
        "    6: \"56-66 years old\",\n",
        "    7: \"67-80 years old\"\n",
        "}\n",
        "\n",
        "# CSV 파일 경로를 저장\n",
        "train_meta_data_path = \"korean_face_age_dataset/custom_train_dataset.csv\"\n",
        "train_meta_data = pd.read_csv(train_meta_data_path)\n",
        "train_image_directory = \"korean_face_age_dataset/train_images\"\n",
        "\n",
        "val_meta_data_path = \"korean_face_age_dataset/custom_val_dataset.csv\"\n",
        "val_meta_data = pd.read_csv(val_meta_data_path)\n",
        "val_image_directory = \"korean_face_age_dataset/val_images\"\n",
        "\n",
        "test_meta_data_path = \"korean_face_age_dataset/custom_test_dataset.csv\"\n",
        "test_meta_data = pd.read_csv(test_meta_data_path)\n",
        "test_image_directory = \"korean_face_age_dataset/test_images\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(128),             # 이미지를 128x128 크기로 리사이즈\n",
        "    transforms.RandomHorizontalFlip(),  # 이미지를 랜덤하게 좌우 반전(Flip)\n",
        "    transforms.ToTensor(),              # 텐서 형식으로 변환\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # RGB 채널에 대한 평균과 표준편차 값\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
        "])\n",
        "\n",
        "# Dataset 객체 생성: 학습용 메타데이터, 이미지 디렉터리, 변환을 인자로 받음\n",
        "train_dataset = Dataset(train_meta_data, train_image_directory, train_transform)\n",
        "\n",
        "# DataLoader 객체 생성: 배치 크기 32로 설정하고, 데이터 셔플을 활성화하여 학습 데이터를 로딩\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = Dataset(val_meta_data, val_image_directory, val_transform)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "test_dataset = Dataset(test_meta_data, test_image_directory, test_transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3f4d11",
      "metadata": {},
      "source": [
        "- 이미지는 보통 [0, 255] 범위의 정수 값으로 표현되지만, ToTensor()는 이를 [0.0, 1.0] 범위의 실수로 변환합니다.\n",
        "- 이미지는 (H, W, C) 형태로 되어있을 수 있는데, ToTensor()는 이를 (C, H, W)로 변환합니다. (C는 채널, H는 높이, W는 너비)\n",
        "- 각 채널에 대해 (image - mean) / std 형태로 정규화가 이루어 집니다.\n",
        "- 정규화하는 이유는 모델이 다양한 입력값 범위를 보다 안정적으로 학습할 수 있도록 돕기 위해서입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db433024",
      "metadata": {},
      "source": [
        "위 코드는 이미지를 128x128 크기로 리사이즈하고, 랜덤하게 좌우 반전을 적용한 뒤, PyTorch Tensor로 변환하고, 마지막으로 각 RGB 채널을 평균 0.5, 표준편차 0.5로 정규화하는 작업을 수행합니다. 이 과정은 이미지 데이터를 딥러닝 모델에 적합한 형태로 전처리하는 데 사용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b4c7b3a",
      "metadata": {
        "id": "8b4c7b3a"
      },
      "source": [
        "### <b>데이터 시각화(Data Visualization)</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c3b6d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "77c3b6d6",
        "outputId": "c66ddc73-c8b5-4477-aef8-211459c66b44"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "plt.rcParams['figure.dpi'] = 70\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "\n",
        "def imshow(input):\n",
        "    # torch.Tensor => numpy 변환 (채널, 높이, 너비 형식에서 높이, 너비, 채널 형식으로 변환)\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    \n",
        "    # 이미지 정규화 되돌리기 (이미지의 평균과 표준편차를 사용하여 원본 값 복원)\n",
        "    mean = np.array([0.5, 0.5, 0.5])  # 각 채널에 대한 평균 값\n",
        "    std = np.array([0.5, 0.5, 0.5])   # 각 채널에 대한 표준편차 값\n",
        "    input = std * input + mean    \n",
        "    # 정규화된 이미지를 복원하기 위해 표준편차와 평균을 되돌림\n",
        "    \n",
        "    # 이미지 값이 0과 1 사이로 제한되도록 클리핑 \n",
        "    # (픽셀 값이 0보다 작거나 1보다 크지 않도록)\n",
        "    input = np.clip(input, 0, 1)\n",
        "    \n",
        "    plt.imshow(input)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# load a batch of train image\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "# visualize a batch of train image\n",
        "imgs, labels = next(iterator)\n",
        "out = torchvision.utils.make_grid(imgs[:6])\n",
        "imshow(out)\n",
        "print([label_to_age[labels[i].item()] for i in range(6)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec27f33f",
      "metadata": {
        "id": "ec27f33f"
      },
      "source": [
        "### 딥러닝 모델 학습\n",
        "\n",
        "- ResNet-50 모델을 생성하는 코드입니다. ResNet-50은 50개의 층을 가진 딥러닝 모델로, 주로 이미지 분류 작업에 사용됩니다.\n",
        "- pretrained=True: pretrained=True는 모델이 ImageNet 데이터셋으로 미리 학습된 가중치를 로드하도록 지정하는 옵션입니다. 즉, 이 모델은 이미 ImageNet 데이터셋을 기반으로 학습된 상태에서 로드되며, 이를 \"전이 학습\"이라고 합니다.\n",
        "- 이렇게 미리 학습된 모델을 사용하면, 동일한 종류의 이미지 분류 문제를 해결할 때 학습을 더 빨리 진행할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd634c41",
      "metadata": {
        "id": "bd634c41"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "log_step = 20\n",
        "\n",
        "# ResNet-50 모델을 생성. ResNet-50은 50개의 층을 가진 딥러닝 모델로, 주로 이미지 분류 작업에 사용\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# 전이 학습을 위한 마지막 레이어 수정\n",
        "# 모델의 마지막 fully connected (fc) 레이어를 새로운 nn.Linear 레이어로 교체\n",
        "# num_features는 모델의 이전 레이어에서 출력된 특성의 개수, 8은 새로운 출력 클래스의 수 (예: 8개의 클래스 분류)\n",
        "model.fc = nn.Linear(num_features, 8) \n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e068d86",
      "metadata": {},
      "source": [
        "train()함수는 주어진 훈련 데이터에 대해 모델을 훈련시키며, 각 배치마다 손실과 정확도를 누적하고, 주어진 간격마다 현재 훈련 상태를 출력합니다. 에포크가 끝나면 최종 훈련 손실과 정확도를 출력하고, 훈련이 끝난 후에는 평균 손실과 정확도를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "004b137b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epoch=0\n",
        "\n",
        "def train():\n",
        "    start_time = time.time()\n",
        "    print(f'[Epoch: {epoch + 1} - Training]')\n",
        "    model.train()\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):  \n",
        "        #훈련 데이터 로더에서 배치 단위로 데이터를 가져옵니다. \n",
        "        imgs, labels = batch\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(imgs)     \n",
        "        #모델에 이미지를 입력하고 예측 결과(outputs)를 얻습니다.\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # 모델의 출력(outputs)에서 클래스별 확률이 가장 높은 값을 선택하여 \n",
        "        # 예측 클래스(preds)를 추출\n",
        "        _, preds = torch.max(outputs, 1)   \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total += labels.shape[0] # 배치에서 처리한 샘플의 수를 누적, 샘플의 개수\n",
        "        running_loss += loss.item() # 배치 손실을 누적, Python 숫자로 변환\n",
        "        \n",
        "    # 예측한 값(preds)과 실제 레이블(labels.data)이 일치하는 샘플의 수를 누적\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        \n",
        "        \n",
        "        # 특정 배치마다 훈련 상태를 출력합니다. \n",
        "        # log_step은 훈련 상태를 출력할 간격을 설정하는 변수로, \n",
        "        # 예를 들어 10으로 설정하면 10번째 배치마다 출력됩니다.\n",
        "        if i % log_step == log_step - 1:\n",
        "            \n",
        "            # 훈련 손실과 훈련 정확도를 출력합니다. \n",
        "            # 손실은 누적 손실을 총 샘플 수로 나눈 값이고, \n",
        "            # 정확도는 예측이 맞은 샘플 수를 총 샘플 수로 나눈 값입니다.\n",
        "            print(f'[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}')\n",
        "    \n",
        "    print(f'train loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
        "    print(\"elapsed time:\", time.time() - start_time)\n",
        "    return running_loss / total, (running_corrects / total).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39940972",
      "metadata": {
        "id": "39940972"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def validate():\n",
        "    start_time = time.time()\n",
        "    print(f'[Epoch: {epoch + 1} - Validation]')\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        imgs, labels = batch\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        total += labels.shape[0]\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if (i == 0) or (i % log_step == log_step - 1):\n",
        "            print(f'[Batch: {i + 1}] running val loss: {running_loss / total}, running val accuracy: {running_corrects / total}')\n",
        "\n",
        "    print(f'val loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
        "    print(\"elapsed time:\", time.time() - start_time)\n",
        "    return running_loss / total, (running_corrects / total).item()\n",
        "\n",
        "\n",
        "def test():\n",
        "    start_time = time.time()\n",
        "    print(f'[Test]')\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for i, batch in enumerate(test_dataloader):\n",
        "        imgs, labels = batch\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(imgs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        total += labels.shape[0]\n",
        "        running_loss += loss.item()\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if (i == 0) or (i % log_step == log_step - 1):\n",
        "            print(f'[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}')\n",
        "\n",
        "    print(f'test loss: {running_loss / total}, accuracy: {running_corrects / total}')\n",
        "    print(\"elapsed time:\", time.time() - start_time)\n",
        "    return running_loss / total, (running_corrects / total).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c21ba656",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습률 초기화 (예시: 0.001로 설정)\n",
        "learning_rate = 0.001\n",
        "\n",
        "# 학습률 조정 함수\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 3:    \n",
        "        lr /= 10      # 3번째 epoch 이후 학습률을 10배 감소\n",
        "    if epoch >= 7:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr     # optimizer의 학습률을 업데이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21aeab28",
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "best_val_acc = 0\n",
        "best_epoch = 0\n",
        "\n",
        "history = []\n",
        "accuracy = []\n",
        "\n",
        "# for epoch in range(num_epochs): num_epochs만큼 반복하면서 훈련을 진행합니다. \n",
        "# epoch는 현재 훈련의 에포크 번호를 나타내며, 0부터 num_epochs-1까지 반복됩니다.\n",
        "for epoch in range(num_epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train_loss, train_acc = train()\n",
        "    val_loss, val_acc = validate()\n",
        "    history.append((train_loss, val_loss))\n",
        "    accuracy.append((train_acc, val_acc))\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        print(\"[Info] best validation accuracy!\")\n",
        "        best_val_acc = val_acc\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), f'best_checkpoint_epoch_{epoch + 1}.pth')\n",
        "\n",
        "torch.save(model.state_dict(), f'last_checkpoint_epoch_{num_epochs}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb9ae9a",
      "metadata": {
        "id": "6fb9ae9a"
      },
      "source": [
        "### <b>학습 결과 확인하기</b>\n",
        "\n",
        "* 학습 결과를 시각화하여 정상적으로 모델이 학습되었는지 확인한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f68367e",
      "metadata": {
        "id": "9f68367e",
        "outputId": "4a8f4471-39c8-427f-d449-45fcd56881e9"
      },
      "outputs": [],
      "source": [
        "plt.plot([x[0] for x in history], 'b', label='train')\n",
        "plt.plot([x[1] for x in history], 'r--',label='validation')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0499a004",
      "metadata": {
        "id": "0499a004",
        "outputId": "b9b27043-42e2-4aef-9b79-23d48886057f"
      },
      "outputs": [],
      "source": [
        "plt.plot([x[0] for x in accuracy], 'b', label='train')\n",
        "plt.plot([x[1] for x in accuracy], 'r--',label='validation')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af413fdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.resnet50(pretrained=True)   \n",
        "# 사전 학습된 ResNet50 모델을 불러옵니다 \n",
        "# pretrained=True를 설정하면, ImageNet 데이터셋에서 학습된 가중치를 가져옵니다.\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "# model.fc는 ResNet50의 마지막 fully connected (fc) layer입니다. in_features는 이 fc layer로 들어오는 입력의 특징 수(즉, 뉴런 수)를 반환합니다. ResNet50의 경우 2048개의 특징이 입력됩니다.\n",
        "\n",
        "model.fc = nn.Linear(num_features, 8) # transfer learning\n",
        "# model.fc를 새로 정의하여, 8개의 클래스로 분류하는 문제에 맞게 출력 뉴런 수를 8로 설정합니다. 즉, 모델의 출력이 8개의 클래스로 분류될 수 있도록 변경한 것입니다.\n",
        "\n",
        "model = model.cuda()\n",
        "# 모델을 GPU로 옮깁니다. cuda()는 모델을 GPU 메모리로 이동시키는 함수로, GPU에서 학습을 진행할 수 있게 해줍니다. 만약 GPU를 사용하지 않는다면 model = model.to('cpu')와 같이 수정할 수 있습니다.\n",
        "\n",
        "model_path = 'best_checkpoint_epoch_6.pth'\n",
        "# 'best_checkpoint_epoch_9.pth' 경로에서 저장된 모델 가중치를 불러옵니다. 이 파일에는 모델의 학습된 가중치가 저장되어 있으며, 이를 model.load_state_dict() 함수를 통해 모델에 적용합니다.\n",
        "\n",
        "# weights_only=True 옵션을 사용하여 모델 가중치 로드\n",
        "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "\n",
        "# 테스트 수행 및 결과 출력\n",
        "test_loss, test_accuracy = test()\n",
        "print(f\"Test loss: {test_loss:.8f}\")\n",
        "print(f\"Test accuracy: {test_accuracy * 100.:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e310415",
      "metadata": {},
      "source": [
        "### **■ PTH 파일?**\n",
        "\n",
        "- best_checkpoint_epoch_**X**.pth 파일은 일반적으로 모델 학습 과정에서 특정 에폭(epoch)에서 가장 좋은 성능을 보인 모델의 가중치를 저장하기 위해 만들어집니다. \n",
        "\n",
        "- 이 파일은 모델의 체크포인트를 저장하여 이후에 모델을 다시 로드하고 평가할 수 있게 합니다. \n",
        "\n",
        "- 이를 통해 모델을 처음부터 학습시키지 않고도 가장 좋은 성능을 보였던 모델 상태를 복원할 수 있습니다.\n",
        "\n",
        "\n",
        "**1) 모델 체크포인트(Storage of Model Checkpoints)** : 머신 러닝 및 딥 러닝 모델 학습 시, 일정한 주기나 조건에서 현재 모델의 상태(가중치, 옵티마이저 상태 등)를 저장합니다. \n",
        "\n",
        "- 이를 체크포인트라고 합니다. 이를 통해 학습이 중단된 경우에도 다시 이어서 학습할 수 있으며, 나중에 성능을 비교하거나 검토할 수 있습니다.\n",
        "\n",
        "- 베스트 모델 저장(Saving the Best Model): 학습 중에 모델의 성능이 평가 지표(예: 검증 데이터셋의 정확도, 손실 등)에 따라 주기적으로 평가됩니다. \n",
        "\n",
        "- 특정 에폭에서 모델이 가장 높은 성능을 보이면 해당 에폭의 모델을 \"베스트 모델\"로 저장할 수 있습니다. \n",
        "\n",
        "- best_checkpoint_epoch_X.pth는 모델이 가장 좋은 성능을 보인 시점, 예를 들어 X번째 에폭의 모델 가중치를 저장한 파일입니다.\n",
        "\n",
        "**2) 실습 예시(Pseudocode Example)** : 모델 학습 코드를 작성할 때, 일반적으로 다음과 같은 로직으로 모델의 체크포인트를 저장합니다:\n",
        "\n",
        "- 따라서, best_checkpoint_epoch_X.pth 파일이 생기는 이유는 X번째 에폭에서 모델이 가장 좋은 성능을 보였고, 해당 시점의 모델 가중치를 저장했기 때문입니다. 이를 통해 나중에 이 모델을 불러와서 테스트하거나 이어서 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c76824b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/ERolmAUw4N5PhhTCyxlERNUB1mDSGZ7GI8KFi80r2aQNlg?download=1 -O saved_best_checkpoint_epoch_8.pth\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd20d216",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = models.resnet50(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 8) # transfer learning\n",
        "model = model.cuda()\n",
        "model_path = 'best_checkpoint_epoch_6.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "test_loss, test_acc = test()\n",
        "print(\"test loss:\", test_loss)\n",
        "print(\"test acc:\", test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7372c3",
      "metadata": {},
      "source": [
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d85dcbd8",
      "metadata": {},
      "source": [
        "## **17.4 Korean Face Age Classificatio 클론 코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4baf4da7",
      "metadata": {},
      "source": [
        "#### <b>Download the Dataset</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7cfdfca",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EbMhBPnmIb5MutZvGicPKggBWKm5hLs0iwKfGW7_TwQIKg?download=1 -O custom_korean_family_dataset_resolution_128.zip\n",
        "!unzip custom_korean_family_dataset_resolution_128.zip -d ./custom_korean_family_dataset_resolution_128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "656352e6",
      "metadata": {},
      "source": [
        "#### <b>Load Libraries</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc9c47e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53cb704a",
      "metadata": {},
      "source": [
        "#### <b>Load Datasets</b>\n",
        "\n",
        "* <b>Training dataset</b>: (F0001 ~ F0299) folders have 10,025 images.\n",
        "* <b>Validation dataset</b>: (F0801 ~ F0850) folders have 1,539 images.\n",
        "* <b>Test dataset</b>: (F0851 ~ F0900) folders have 1,504 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f149cb7a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parsing(meta_data):\n",
        "    image_age_list = []\n",
        "    # iterate all rows in the metadata file\n",
        "    for idx, row in meta_data.iterrows():\n",
        "        image_path = row['image_path']\n",
        "        age_class = row['age_class']\n",
        "        image_age_list.append([image_path, age_class])\n",
        "    return image_age_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acdbaa0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, meta_data, image_directory, transform=None):\n",
        "        self.meta_data = meta_data\n",
        "        self.image_directory = image_directory\n",
        "        self.transform = transform\n",
        "\n",
        "        # process the meta data\n",
        "        image_age_list = parsing(meta_data)\n",
        "\n",
        "        self.image_age_list = image_age_list\n",
        "        self.age_class_to_label = {\n",
        "            \"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3, \"e\": 4, \"f\": 5, \"g\": 6, \"h\": 7\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.meta_data)\n",
        "            \n",
        "    def __getitem__(self, idx):\n",
        "        image_path, age_class = self.image_age_list[idx]\n",
        "        img = Image.open(os.path.join(self.image_directory, image_path))\n",
        "        label = self.age_class_to_label[age_class]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2578d921",
      "metadata": {},
      "outputs": [],
      "source": [
        "label_to_age = {\n",
        "    0: \"0-6 years old\",\n",
        "    1: \"7-12 years old\",\n",
        "    2: \"13-19 years old\",\n",
        "    3: \"20-30 years old\",\n",
        "    4: \"31-45 years old\",\n",
        "    5: \"46-55 years old\",\n",
        "    6: \"56-66 years old\",\n",
        "    7: \"67-80 years old\"\n",
        "}\n",
        "\n",
        "# CSV 파일 경로를 저장\n",
        "train_meta_data_path = \"korean_face_age_dataset/custom_train_dataset.csv\"\n",
        "train_meta_data = pd.read_csv(train_meta_data_path)\n",
        "train_image_directory = \"korean_face_age_dataset/train_images\"\n",
        "\n",
        "val_meta_data_path = \"korean_face_age_dataset/custom_val_dataset.csv\"\n",
        "val_meta_data = pd.read_csv(val_meta_data_path)\n",
        "val_image_directory = \"korean_face_age_dataset/val_images\"\n",
        "\n",
        "test_meta_data_path = \"korean_face_age_dataset/custom_test_dataset.csv\"\n",
        "test_meta_data = pd.read_csv(test_meta_data_path)\n",
        "test_image_directory = \"korean_face_age_dataset/test_images\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
        "])\n",
        "\n",
        "train_dataset = Dataset(train_meta_data, train_image_directory, train_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "val_dataset = Dataset(val_meta_data, val_image_directory, val_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "test_dataset = Dataset(test_meta_data, test_image_directory, test_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ef1d07",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [12, 8]\n",
        "plt.rcParams['figure.dpi'] = 60\n",
        "plt.rcParams.update({'font.size': 20})\n",
        "\n",
        "\n",
        "def imshow(input):\n",
        "    # torch.Tensor => numpy\n",
        "    input = input.numpy().transpose((1, 2, 0))\n",
        "    # undo image normalization\n",
        "    mean = np.array([0.5, 0.5, 0.5])\n",
        "    std = np.array([0.5, 0.5, 0.5])\n",
        "    input = std * input + mean\n",
        "    input = np.clip(input, 0, 1)\n",
        "    # display images\n",
        "    plt.imshow(input)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# load a batch of train image\n",
        "iterator = iter(train_dataloader)\n",
        "\n",
        "# visualize a batch of train image\n",
        "imgs, labels = next(iterator)\n",
        "out = torchvision.utils.make_grid(imgs[:4])\n",
        "imshow(out)\n",
        "print([label_to_age[labels[i].item()] for i in range(4)]) # 1: family, 0: not family"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
