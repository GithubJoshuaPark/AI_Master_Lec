{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8XHHT99-Slm"
   },
   "source": [
    "### Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동미분 및 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaV6bKLx-Sls"
   },
   "source": [
    "> ## 학습 목표\n",
    "\n",
    "-   다양한 손실 함수(예: MSE, Cross-Entropy)의 정의와 목적을 이해하고, 특정 문제(회귀/분류)에 적합한 손실 함수를 선택하여 파이토치로 구현할 수 있다.\n",
    "    \n",
    "-   경사하강법의 기본 원리(기울기 계산 및 가중치 업데이트)를 이해하고, 이를 사용하여 파라미터를 최적화하는 과정을 파이토치로 구현할 수 있다. \n",
    "\n",
    "    또한, 다양한 변형 기법(예: 모멘텀, Adam 등)의 차이점을 설명할 수 있다.\n",
    "    \n",
    "-   자동 미분의 원리를 이해하고, 파이토치의 `autograd` 기능을 활용하여 복잡한 모델의 기울기를 자동으로 계산하는 방법을 이해하고, \n",
    "\n",
    "    이를 통해 손실 함수를 최적화하는 과정에서 자동 미분을 효과적으로 활용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 손실함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 손실 함수란?\n",
    "\n",
    "-   신경망은 \"하나의 지표\"를 기준으로 최적의 매개변수 값을 탐색한다. **신경망 학습에서 사용하는 지표를 손실함수**라고 한다.\n",
    "-   손실 함수는 신경망 성능의 \"나쁨\"을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 \"못\"하느냐를 나타낸다.\n",
    "-   일반적으로 손실함수로 (교차 엔트로피) 와 (오차제곱합)을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 왜 손실함수를 이용하는가?\n",
    "\n",
    "- \"정확도\"라는 지표를 나두고 \"손실함수\"라는 우회적인 방법을 선택하는 이유는 바로 신경망 학습에서의 \"미분\"때문이다. \n",
    "\n",
    "- 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 한 작게 하는 매개변수 값을 찾는다. \n",
    "\n",
    "   이때 매개변수의 미분을 계산하고, 그 미분값을 단서로 매개 변수의 값을 서서히 갱신하는 과정을 반복한다.  \n",
    "\n",
    "- 미분값이 음수면 가중치 매개변수를 양의 방향으로 변환시켜 손실 함수의 값을 줄일 수 있고, 반대로 미분 값이 양수이면 가중치 매개변수를 음의 방향으로 변환시켜 손실 함수의 값을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실함수(Loss funtion)는 모델의 예측 데이터와 실제 데이터와의 거리를 측정하는 함수입니다.\n",
    "- 손실함수는 모델 학습 과정에서 최적화에 사용합니다. \n",
    "- 파이토치에서는 torch.nn.Module을 상속받아 사용자 정의 손실 함수를 만들 수 있습니다.\n",
    "- 손실함수는 회귀 문제에서 평균제곱오차(mean squared error, MSE)를 적용합니다.\n",
    "- 분류 문제는 크로스엔트로피오차(cross entropy error, CEE)를 적용합니다.\n",
    "- MSE는 예측값과 실제 값의 차이를 제곱하여 평균을 내는 방식으로 손실을 계산합니다.\n",
    "- CEE는 실제 클래스와 예측 클래스 확률 분포 간의 차이를 측정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 사용자 정의 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **평균 제곱 오차(Mean Square Error, MSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward 메서드에서는 모델의 출력과 실제 값을 받아 오차를 계산합니다. \n",
    "- outputs와 targets 사이의 차이를 제곱하여 평균을 구하는 **평균 제곱 오차(MSE)** 를 사용했습니다.\n",
    "- 실제 데이터와 예측 데이터 차이(=오차)의 제곱에 대해 평균을 취한 것\n",
    "- 입력 값이 여러 개인 경우, 가설을 세우고 값이 조건에 충족하는지 판단해서 조금씩 수정해 나가면서 오차가 최소가 될 때까지 반복하는 방법 사용\n",
    "- 값을 조금씩 수정해 나가는 과정에서 오차를 평가하는 방법이 평균 제곱 오차\n",
    "- 평균 제곱 오차에서 구한 값을 바탕으로 오차 최소화되는 값 구해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300\" height=\"\" src=\"./image/mse.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hat yi : 신경망의 출력(신경망이 추정한 값), yi : 실제값, N:데이터 차원의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 각 데이터에서 오차를 구한다\n",
    "\n",
    "2. 각 오차 값을 제곱한 후, 모두 더한다. (부호가 있으면 정확한 오차 구하기 힘드므로 제곱 사용)\n",
    "\n",
    "3. 오차의 제곱 합을 원소의 총 개수로 나눈다. (오차의 합을 n(원소의 총 개수)로 나눈 것)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. y = 3x + 76이라는 직선의 방정식을 가설로 세웠을 때, 오차는\n",
    "\n",
    "<table style=\"border-collapse: collapse; width: 99.5349%; height: 143px;\" border=\"1\" data-ke-align=\"alignLeft\"><tbody><tr style=\"height: 19px;\"><td style=\"width: 20%; text-align: center; height: 19px;\">공부한 시간</td><td style=\"width: 20%; text-align: center; height: 19px;\">2</td><td style=\"width: 20%; text-align: center; height: 19px;\">4</td><td style=\"width: 20%; text-align: center; height: 19px;\">6</td><td style=\"width: 20%; text-align: center; height: 19px;\">8</td></tr><tr style=\"height: 19px;\"><td style=\"width: 20%; text-align: center; height: 19px;\">성적</td><td style=\"width: 20%; text-align: center; height: 19px;\">81</td><td style=\"width: 20%; text-align: center; height: 19px;\">93</td><td style=\"width: 20%; text-align: center; height: 19px;\">91</td><td style=\"width: 20%; text-align: center; height: 19px;\">97</td></tr><tr style=\"height: 19px;\"><td style=\"width: 20%; text-align: center; height: 19px;\">예측 값</td><td style=\"width: 20%; text-align: center; height: 19px;\">82</td><td style=\"width: 20%; text-align: center; height: 19px;\">88</td><td style=\"width: 20%; text-align: center; height: 19px;\">94</td><td style=\"width: 20%; text-align: center; height: 19px;\">100</td></tr><tr style=\"height: 19px;\"><td style=\"width: 20%; text-align: center; height: 19px;\">오차(실제 값 - 예측 값)</td><td style=\"width: 20%; text-align: center; height: 19px;\">1</td><td style=\"width: 20%; text-align: center; height: 19px;\">-5</td><td style=\"width: 20%; text-align: center; height: 19px;\">3</td><td style=\"width: 20%; text-align: center; height: 19px;\">3</td></tr></tbody></table>\n",
    "\n",
    "2\\. 각 오차의 값을 제곱한 후, 모두 더하면\n",
    "\n",
    "> 1 + 25 + 9 + 9 = 44\n",
    "\n",
    "3\\. 오차의 제곱합을 원소의 총 개수로 나누면\n",
    "\n",
    "> 1/4 \\* 44 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이 라이브러리 불러오기\n",
    "import numpy as np\n",
    "\n",
    "# 가상의 기울기 a와 y절편 b를 정하기\n",
    "fake_a=3\n",
    "fake_b=76\n",
    "\n",
    "# 공부 시간 x와 성적 y의 넘파이 배열을 만듦\n",
    "x = np.array([2, 4, 6, 8])\n",
    "y = np.array([81, 93, 91, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=ax + b에 가상의 a,b 값을 대입한 결과를 출력하는 함수 생성\n",
    "def predict(x):\n",
    "    return fake_a * x + fake_b\n",
    "\n",
    "# 예측 값이 들어갈 빈 리스트를 만듦\n",
    "predict_result = []\n",
    "\n",
    "# 모든 x값을 한 번씩 대입하여 predict_result 리스트를 완성\n",
    "for i in range(len(x)):\n",
    "    predict_result.append(predict(x[i]))\n",
    "    print(\"공부시간=%.f, 실제점수=%.f, 예측점수=%.f\" % (x[i], y[i], predict(x[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 제곱 오차 함수를 각 y값에 대입하여 최종 값을 구하는 함수\n",
    "n=len(x)\n",
    "def mse(y, y_pred):\n",
    "    return (1/n) * sum((y - y_pred)**2)\n",
    "\n",
    "# 평균 제곱 오차 값을 출력\n",
    "print(\"평균 제곱 오차: \" + str(mse(y,predict_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# nn.Module 상속받음\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # MSE (Mean Squared Error) 손실 함수 구현\n",
    "        loss = torch.mean((outputs - targets) ** 2)\n",
    "        # torch.mean과 출력값과 정답과의 차이의 제곱 연산\n",
    "        return loss   # 손실함수 값 반환한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 교차 엔트로피 오차(Cross Entropy Error, CEE)\n",
    "- F.softmax(outputs, dim=1)에서 outputs은 로짓(logits)값으로 소프트맥스 통해 확률 값으로 변환합니다.\n",
    "- torch.log(target_probs) 정답 클래스의 확률에 대해 로그를 취하고, 음수 부호를 붙입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교차 엔트로피 오차의 수식에서 log는 밑이 e인 자연로그이다. t는 원-핫 인코딩으로 표현된 정답 레이블이므로, 결국 실제로 정답일 때 (t=1)의 softmax 추정 값에 대한 자연로그를 계산하는 식이 된다. \n",
    "\n",
    "예를 들어, 정답 레이블이 2이고, 신경망의 softmax 출력 값이 0.6이면 교차 엔트로피 오차는 -log0.6=0.51이 된다. \n",
    "\n",
    "같은 조건에서 신경망의 출력 값이 0.1이면 -log0.1=2.30이 된다. 결론적으로, 교차 엔트로피 오차는 정답일 때의 신경망 출력 값이 전체 값이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x=1일 때 y=0이고, x가 0에 가까워 질수록 y는 작아진다. \n",
    "\n",
    "이것을 교차 엔트로피 오차의 수식에 적용하여 마이너스를 붙이면 반대가 된다. 즉, 예측 값이 커질수록 오차는0에 가까워지고, 예측 값이 작아질수록 오차는 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300\" height=\"\" src=\"./image/cee.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # 소프트맥스를 적용하여 확률 분포로 변환\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "        # 각 샘플에 대해 정답 클래스의 확률 값을 추출\n",
    "        # targets 정수 레이블, gather 확률 추출\n",
    "        target_probs = probs.gather(1, targets.view(-1, 1))\n",
    "\n",
    "        # 크로스 엔트로피 손실 계산\n",
    "        loss = -torch.log(target_probs).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 3개의 클래스와 5개의 샘플\n",
    "outputs = torch.randn(5, 3)\n",
    "# 모델의 예측 값 (5개의 샘플, 3개의 클래스에 대한 로짓)\n",
    "targets = torch.randint(0, 3, (5,))\n",
    "# 실제 클래스 (정수 클래스 레이블, 5개 샘플)\n",
    "\n",
    "# 사용자 정의 크로스 엔트로피 손실 함수 사용\n",
    "loss_fn = CustomCrossEntropyLoss()\n",
    "loss = loss_fn(outputs, targets)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 파이토치에서 제공하는 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평균 제곱 오차(MSE) 손실 함수 사용\n",
    "\n",
    "  `loss_fn = nn.MSELoss()`\n",
    "\n",
    "  `loss = loss_fn(outputs, targets)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 실제 값과 예측 값\n",
    "y_true = torch.tensor([3.0, 5.0, 2.0])\n",
    "y_pred = torch.tensor([2.5, 4.5, 2.0])\n",
    "\n",
    "# MSELoss 객체 생성\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# 손실 계산\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(f'Mean Squared Error: {loss.item():.4f}')  # 손실 값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 평균제곱오차를 정의하면 아래와 같다.\n",
    "\n",
    "# 손실함수로 평균제곱오차 정의\n",
    "# y_pred: 예측값 (모델의 출력)\n",
    "# y_true: 실제값 (정답)\n",
    "def mean_squared_error(y_pred, y_true): # 손실함수가 예측값과 실제값을 입력받는다.\n",
    "    return torch.mean((y_pred - y_true) ** 2)\n",
    "# 두 값의 차이의 제곱에 평균을 계산하여 손실값을 반환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크로스 엔트로피 오차 nn.CrossEntropyLoss()\n",
    "- CrossEntropyLoss는 소프트맥스와 로그 손실을 한 번에 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 3개의 클래스와 5개의 샘플\n",
    "outputs = torch.randn(5, 3)\n",
    "# 모델의 예측 값 (5개의 샘플, 3개의 클래스에 대한 로짓)\n",
    "targets = torch.randint(0, 3, (5,))\n",
    " # 실제 클래스 (0~2까지의 정수 클래스 레이블, 5개 샘플)\n",
    "\n",
    "# 내장된 CrossEntropyLoss 사용\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, targets)  # 손실 계산\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **손실 함수를 설정하는 이유**\n",
    "\n",
    "**신경망 학습에서의 '미분' 역할**에 있다.\n",
    "\n",
    "신경망 학습에서 최적의 매개변수를 탐색할 때, 매개변수에 대해 미분(기울기)을 계산하고 미분 값을 갱신하는 과정을 반복함으로써 손실 함수의 값을 가능한 작게 하는 매개변수를 찾는다.\n",
    "\n",
    "미분 값이 양수이면 매개변수를 음의 방향으로 갱신하고, 음수이면 양의 방향으로 갱신한다.\n",
    "\n",
    "반면, 정확도를 지표로 삼으면 대부분의 미분 값이 0이 이므로 갱신할 수 없다. \n",
    "\n",
    "예를 들어, 100장의 데이터 중 30장을 제대로 인식한다고 가정하면, 정확도는 30%이다. \n",
    "\n",
    "이때 매개변수 값을 미세하게만 조정한다면 정확도가 개선되지 않고 일정할 것이다. \n",
    "\n",
    "왜냐하면 30%, 31%, 32%와 같이 불연속적으로 개선되고 30.0133%와 같이 조정되지는 않기 때문이다. \n",
    "\n",
    "즉, 손실함수는 매개변수의 값이 조금 변하면 그에 연속적인 값으로 반응하지만, 정확도는 거의 반응을 보이지 않는다. \n",
    "\n",
    "★ 신경망 학습에서는 기울기가 0이 되지 않는 덕분에 올바르게 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 경사하강법 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경사하강법(gradient descent)은 딥러닝 모델을 훈련하는 데 중요한 최적화 알고리즘입니다.\n",
    "- 경사하강법은 손실 함수가 최소가 되는 방향으로 모델 파라미터(W, b)를 업데이트하여 점진적으로 최적화합니다.\n",
    "- 학습률과 배치 크기 같은 하이퍼파라미터를 설정하는 것은 경사하강법의 성능에 영향을 주는 중요한 요소입니다.\n",
    "- torch.optim에는 SGD, Adam 등 최적화 알고리즘이 구현되어 있습니다. \n",
    "- Adam 옵티마이저는 두 가지 모멘텀으로 파라미터 업데이트를 실행합니다. 1차 모멘텀(moving average of gradients): 기울기(gradient)의 평균을 추적하여 파라미터 업데이트에서 일관성을 유지합니다. 2차 모멘텀(moving average of squared gradients): 기울기의 제곱값의 평균을 추적하여 학습률을 적응적으로 조절합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/3.2_경사하강법 최소지점찾기.png\" width=\"500\">\n",
    "<figcaption>그림 3.2 경사하강법 최소지점찾기</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/3.3_grad.png\" width=\"500\">\n",
    "<figcaption>그림 3.3 경사하강법</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 SGD(확률적 경사하강법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.optim.SGD를 사용하여 경사하강법을 적용할 수 있습니다. \n",
    "- SGD는 모델의 파라미터를 최적화하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# 모델 정의\n",
    "model = torch.nn.Linear(10, 2)  # 10개의 입력 특성, 2개의 출력 클래스\n",
    "\n",
    "# 손실 함수 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 정의 (확률적 경사하강법)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 데이터\n",
    "inputs = torch.randn(64, 10)  # 64개의 샘플, 10개의 입력 특성\n",
    "targets = torch.randint(0, 2, (64,))  # 64개의 샘플, 2개의 클래스 레이블\n",
    "\n",
    "# 학습 과정\n",
    "optimizer.zero_grad()  # 기울기 초기화\n",
    "outputs = model(inputs)  # 모델의 출력 계산\n",
    "loss = criterion(outputs, targets)  # 손실 계산\n",
    "loss.backward()  # 기울기 계산\n",
    "optimizer.step()  # 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 경사하강법 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습률(learning rate)은 경사하강법에서 중요한 하이퍼파라미터 중 하나입니다. \n",
    "- 학습률이 너무 크면 최적점에 도달하지 못하고 발산할 수 있고, 너무 작으면 학습이 너무 느려질 수 있습니다.\n",
    "- 경사하강법은 손실함수의 지역 최소값(local minimum)에 빠질 수 있습니다. \n",
    "- 지역 최솟값은 최적의 해가 아니며 클로벌 최솟값을 찾아야합니다. 여기에 사용하는 최적화 기법이 Adam입니다.\n",
    "- 배치 크기(batch size)에 따라 학습 속도와 정확도가 달라집니다. 작은 배치를 사용하면 더 자주 업데이트 되지만 불안정할 수 있고, 큰 배치를 사용하면 안정적이지만 계산 비용이 더 많이 듭니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `경사하강법을 이용한 모델 학습 실습`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 생성\n",
    "- rand 0~1 사이 랜덤 값 100개 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 데이터 생성\n",
    "# 선형 데이터 y = 2x + 1 (노이즈 추가)\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(100, 1).astype(np.float32)\n",
    "y_train = 2 * X_train + 1 + np.random.normal(0, 0.1, (100, 1)).astype(np.float32)\n",
    "\n",
    "# 2. 파이토치 텐서로 변환\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "# 3. 모델 정의\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # 선형 계층 정의 (입력 1, 출력 1)\n",
    "        self.linear = nn.Linear(1, 1)  # y = wx + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 모델 객체 생성\n",
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 4. 손실 함수 및 최적화 알고리즘 설정\n",
    "criterion = nn.MSELoss()  # 평균 제곱 오차\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # 경사 하강법 (학습률 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epochs만큼 학습을 진행하며, 매 epoch마다 손실 값을 출력하고, 손실 값이 기록됩니다.\n",
    "- optimizer.zero_grad()는 이전의 기울기를 초기화하고, \n",
    "- loss.backward()는 역전파를 수행하여 기울기를 계산하며, \n",
    "- optimizer.step()은 파라미터(가중치와 편향)를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 5. 모델 학습\n",
    "epochs = 1000  # 학습 epoch 수\n",
    "losses = []  # 손실 기록\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 1) 순전파\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # 2) 손실 계산\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    # 3) 기울기 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4) 역전파\n",
    "    loss.backward()\n",
    "    # 5) 가중치 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    # 손실 기록\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 6. 학습된 모델 시각화\n",
    "# 손실값의 변화, 학습된 직선과 데이터를 시각화\n",
    "plt.plot(losses)\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# 7. 학습된 모델의 가중치와 편향 출력\n",
    "print(f\"학습된 기울기 (w): {model.linear.weight.item():.4f}\")\n",
    "print(f\"학습된 편향 (b): {model.linear.bias.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그래프는 학습이 진행될 때마다 손실 값이 감소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 8. 모델 예측\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_train)\n",
    "\n",
    "# 학습 데이터와 예측값 시각화\n",
    "plt.scatter(X_train.numpy(), y_train.numpy(), label='True data')\n",
    "plt.plot(X_train.numpy(), predictions.numpy(), color='red', label='pred line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adam 을 적용할때는 아래와 같이 코드를 작성할 수 있다.\n",
    "- model = SimpleNN()\n",
    "- criterion = nn.BCELoss()  # 이진 분류의 경우 BCELoss 사용\n",
    "- optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 자동 미분의 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝 모델은 오차를 줄이기 위해 손실함수(loss function)를 적용하고 최적화 알고리즘으로 경사하강법을 사용합니다. \n",
    "- 경사하강법은 미분으로 함수의 변화율을 측정하며 곡선의 그래프에서 기울기(gradient)를 계산하여 오차의 최솟값 방향으로 이동을 하면서 가중치    최적화를 찾는 알고리즘 입니다.\n",
    "- 파이토치에서는 이러한 알고리즘에 대하여 오토그래드(Autograd)를 사용하여 자동미분을 제공하고 있습니다.\n",
    "- 신경망을 학습할 때 가중치 업데이트를 위해 필요한 기울기를 자동으로 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Autograd의 기본 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Autograd는 딥러닝 모델에서 기울기를 자동으로 계산하고 학습을 하며 동적 계산 그래프를 사용하여 미분을 처리합니다.\n",
    "- **requires_grad=True** 로 설정된 텐서는 연산에 대해 기울기를 계산할 수 있도록 추적됩니다.\n",
    "- 예시: x = torch.tensor([1.0], requires_grad=True)\n",
    "- 텐서 간의 연산이 이루어질 때마다 자동으로 계산 그래프에 추가됩니다. \n",
    "- y = x * 2라는 연산은 x와 2를 입력으로 받아 y라는 새로운 텐서를 생성하고 계산 그래프에 기록됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# requires_grad=True로 설정\n",
    "# 스칼라값\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# 2차 함수 정의\n",
    "y = x**2\n",
    "\n",
    "# 역전파 호출하여 기울기 계산하고 grad에 저장\n",
    "# y를 x에 대해 미분\n",
    "y.backward()\n",
    "\n",
    "# x에 대한 기울기 출력\n",
    "print(f'기울기: {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **기울기는 딥러닝에서 손실 함수에 대한 파라미터(가중치)의 미분을 의미**합니다. \n",
    "- 기울기를 계산하고 파라미터를 업데이트함으로써 모델을 학습시킬 수 있습니다.\n",
    "- 기울기를 계산하는 과정에서 x.grad에 값이 저장되므로, 반복적인 학습을 위해서는 각 단계에서 기울기를 초기화해야 합니다. \n",
    "- optimizer.zero_grad()나 x.grad.zero_()를 사용하여 기울기를 수동으로 초기화할 수 있습니다.\n",
    "- 기울기 비활성화는 torch.no_grad()를 사용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **기울기 비활성화**\n",
    "  \n",
    "  메모리 절약: 기본적으로 PyTorch는 모든 연산에 대해 기울기를 계산하여 역전파(backpropagation)에 사용할 수 있게 합니다. \n",
    "  with torch.no_grad()를 사용하면, 연산이 그래프에 기록되지 않기 때문에, 기울기를 계산하고 저장할 필요가 없어 메모리 사용량이 절약됩니다.\n",
    "\n",
    "\n",
    "- **연산 속도 향상**\n",
    "\n",
    "  기울기 계산을 생략함으로써 역전파를 위한 추가적인 계산이 이루어지지 않습니다. 이로 인해 연산 속도가 빨라집니다. \n",
    "  \n",
    "  이는 주로 추론 단계에서 더 큰 차이를 만들어냅니다. 특히 큰 모델이나 배치 크기가 클 때 성능 차이가 눈에 띄게 나타날 수 있습니다.\n",
    "\n",
    "\n",
    "- **불필요한 그래프 저장 방지**\n",
    "\n",
    "  Autograd 그래프는 기본적으로 PyTorch에서 계산 그래프를 생성하여 후속 연산을 위한 기울기를 계산합니다. \n",
    "  \n",
    "  이 그래프는 메모리를 많이 차지할 수 있습니다. torch.no_grad() 블록 내에서는 그래프가 생성되지 않으므로, 불필요한 그래프 저장을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# requires_grad=True로 텐서 생성\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**2  # y = x^2\n",
    "z = y.sum()  # z = y의 합\n",
    "\n",
    "# 역전파를 통해 기울기 계산\n",
    "z.backward()  # dz/dx 계산 z = x^2이므로 dz/dx = 2x이고, x가 2일 때, 기울기는 4이다.\n",
    "print(x.grad)  # x에 대한 기울기 출력\n",
    "\n",
    "# x의 기울기 초기화\n",
    "x.grad.zero_()\n",
    "\n",
    "# 기울기 비활성화\n",
    "with torch.no_grad():\n",
    "    y = x**2\n",
    "    z = y.sum()\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `그라디언트 계산 실습`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x와 w 텐서에 대해 연산이 진행되면, 파이토치는 이를 계산하는 그래프를 생성합니다.\n",
    "- z = y.sum()을 호출하면 이 값은 스칼라이기 때문에, z.backward()를 통해 역전파가 수행되고 각 텐서에 대한 기울기가 계산됩니다.\n",
    "- x.grad와 w.grad에는 각 텐서에 대한 기울기가 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 입력 텐서 x와 파라미터 가중치 텐서 w를 생성 (requires_grad=True로 설정)\n",
    "x = torch.randn(5, 5, requires_grad=True)  # 5x5 랜덤 텐서\n",
    "w = torch.randn(5, 5, requires_grad=True)  # 5x5 랜덤 텐서\n",
    "\n",
    "# y = x * w 연산\n",
    "y = x * w\n",
    "\n",
    "# y의 모든 원소에 대해 합을 구하는 스칼라 값 계산\n",
    "z = y.sum()\n",
    "print(z)\n",
    "\n",
    "# 역전파 호출 기울기 계산\n",
    "z.backward()\n",
    "\n",
    "# w와 x에 대한 기울기 출력\n",
    "print(f\"x의 기울기: {x.grad}\")\n",
    "print(f\"w의 기울기: {w.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#1. x는 10행 1열 텐서\n",
    "# randn 정규분포에서 난수 추출\n",
    "x = torch.randn(10, 1)\n",
    "y = 3 * x + 2\n",
    "\n",
    "#2. 매개변수 가중치 w와 편향 b\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# 학습률\n",
    "lr = 0.01\n",
    "\n",
    "# 학습 반복\n",
    "for epoch in range(100):\n",
    "    # 예측 값\n",
    "    y_pred = x * w + b\n",
    "\n",
    "    # 손실 계산 (MSE)\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "\n",
    "    # 역전파 호출하여 기울기 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 파라미터 업데이트\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "\n",
    "    # 기울기 초기화\n",
    "    w.grad.zero_() # w 텐서의 기울기를 0으로 초기화\n",
    "    b.grad.zero_() # x 텐서의 기울기를 0으로 초기화\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
