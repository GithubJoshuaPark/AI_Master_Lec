{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuEC8iuTv0MI"
   },
   "source": [
    "# 딥러닝 및 머신러닝 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh9fMM8nv0ML"
   },
   "source": [
    "> ## 학습 목표\n",
    "\n",
    "- AI(인공지능), ML(머신러닝), DL(딥러닝)의 개념과 차이점을 이해한다.\n",
    "- 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning), 강화학습(Reinforcement Learning)의 차이를 이해한다.\n",
    "- 신경망(Neural Network)의 기본 작동 원리(입력, 가중치, 편향, 활성화 함수, 역전파 등)와 학습 내용을 직접 실습, 구현할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7FAWRY6v0MM"
   },
   "source": [
    "## 1.1 AI-ML-DL의 구조 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 인공지능(AI, Artificial Intelligence)\n",
    "  \n",
    "  > #### 정의\n",
    "  -   인간의 지능을 모방하는 컴퓨터 시스템, 학습, 추론, 문제 해결, 언어 이해, 인지 등의 능력을 포함\n",
    "  \n",
    "   - 목표: 인간처럼 사고하고 학습하는 기계 구현   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 머신러닝(ML, Machine Learning)\n",
    "  \n",
    "#### AI의 하위 분야, 데이터를 학습하여 예측하거나 결정을 내리는 알고리즘을 개발하는 것을 의미\n",
    "  \n",
    "  \n",
    " ### 1\\. 지도학습 (Supervised Learning)\n",
    "\n",
    "> #### 정의\n",
    "\n",
    "-   **레이블이 있는 데이터**를 학습하여 **입력과 출력 간의 관계**를 파악.\n",
    "\n",
    "> #### 예시\n",
    "\n",
    "-   **입력**: 키와 몸무게 데이터 (특징).\n",
    "-   **출력**: 성별 (레이블).\n",
    "-   **알고리즘**:\n",
    "    -   의사결정 트리 (Decision Tree)\n",
    "    -   서포트 벡터 머신 (SVM)\n",
    "    -   합성곱 신경망 (CNN)\n",
    "\n",
    "> #### 과정\n",
    "\n",
    "1.  **학습 단계**\n",
    "    -   모델이 입력 데이터를 학습하여 레이블(정답)을 예측할 수 있는 능력을 만듦.\n",
    "2.  **테스트 단계**\n",
    "    -   학습된 모델을 사용해 새로운 데이터에 대해 결과를 예측.\n",
    "    -   모델의 성능을 평가.\n",
    "\n",
    "___\n",
    "\n",
    "### 2\\. 비지도학습 (Unsupervised Learning)\n",
    "\n",
    "> #### 정의\n",
    "\n",
    "-   **레이블이 없는 데이터**를 분석하여 **패턴 인식, 군집화, 차원 축소** 등의 작업 수행.\n",
    "\n",
    "> #### 주요 응용 분야\n",
    "\n",
    "-   데이터 군집화 (Clustering).\n",
    "-   차원 축소 (Dimensionality Reduction).\n",
    "\n",
    "___\n",
    "\n",
    "### 3\\. 강화학습 (Reinforcement Learning)\n",
    "\n",
    "> #### 정의\n",
    "\n",
    "-   **에이전트(Agent)가** 환경(Environment)과 상호작용하며, **보상(Rewards)을** 통해 학습.\n",
    "\n",
    "> #### 목표\n",
    "\n",
    "-   최적 행동 방식을 학습하여 **장기적으로 최대 보상**을 얻는 전략 수립.\n",
    "\n",
    "> #### 예시\n",
    "\n",
    "-   게임 AI (예: 알파고).\n",
    "-   자율 주행 자동차."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/1.1_머신러닝 학습.png\" width=\"500\" height=\"\" >\n",
    "<figcaption>그림 1.1 머신러닝 학습</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scikit-Learn 코드:\n",
    "\n",
    "  - 빠르고 간단하며 직관적임.\n",
    "  - 초보자에게 추천.\n",
    "  - 복잡한 신경망을 필요로 하지 않는 간단한 지도학습 문제에 적합.\n",
    "\n",
    "- PyTorch 코드:\n",
    "\n",
    "  - 딥러닝을 배우거나, 더 복잡한 신경망 모델을 정의하고 학습할 때 유용.\n",
    "  - 학습 데이터셋의 형태를 명확히 이해하며, 앞으로 GPU 환경에서도 쉽게 확장 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[파이썬과 사이킷런활용 예제]\n",
    "\n",
    "- 기본적인 머신러닝 프로세스를 잘 보여주며, 파이썬과 사이킷런을 이용한 데이터 분석, 모델 학습 및 평가의 패턴을 익힐 수 있는 좋은 예입니다. \n",
    "- 다양한 모델과 데이터셋에 대해 이와 유사한 과정을 반복하며 실습하면 머신러닝의 기초를 더 확고히 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 24.2 from /opt/anaconda3/lib/python3.12/site-packages/pip (python 3.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: done\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      " - pytorch\n",
      "Platform: osx-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - numpy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2025.1.31          |     pyhd8ed1ab_0         159 KB  conda-forge\n",
      "    conda-25.3.0               |  py312hb401068_0         1.1 MB  conda-forge\n",
      "    libexpat-2.6.3             |       hac325c4_0          69 KB  conda-forge\n",
      "    libsqlite-3.46.0           |       h1b8f9f3_0         887 KB  conda-forge\n",
      "    libzlib-1.2.13             |       h87427d6_6          56 KB  conda-forge\n",
      "    openssl-3.4.1              |       hc426f3f_0         2.5 MB  conda-forge\n",
      "    python-3.12.2              |h9f0c242_0_cpython        13.9 MB  conda-forge\n",
      "    python_abi-3.12            |          5_cp312           6 KB  conda-forge\n",
      "    zlib-1.2.13                |       h87427d6_6          87 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        18.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  libexpat           conda-forge/osx-64::libexpat-2.6.3-hac325c4_0 \n",
      "  libsqlite          conda-forge/osx-64::libsqlite-3.46.0-h1b8f9f3_0 \n",
      "  libzlib            conda-forge/osx-64::libzlib-1.2.13-h87427d6_6 \n",
      "  python_abi         conda-forge/osx-64::python_abi-3.12-5_cp312 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              pkgs/main::conda-24.11.3-py312hecd8cb~ --> conda-forge::conda-25.3.0-py312hb401068_0 \n",
      "  openssl              pkgs/main::openssl-3.0.16-h184c1cd_0 --> conda-forge::openssl-3.4.1-hc426f3f_0 \n",
      "  zlib                    pkgs/main::zlib-1.2.13-h4b97444_1 --> conda-forge::zlib-1.2.13-h87427d6_6 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/osx-64::certifi-2025.1.31-p~ --> conda-forge/noarch::certifi-2025.1.31-pyhd8ed1ab_0 \n",
      "  python                pkgs/main::python-3.12.7-hcd54a6c_0 --> conda-forge::python-3.12.2-h9f0c242_0_cpython \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "python-3.12.2        | 13.9 MB   |                                       |   0% \n",
      "openssl-3.4.1        | 2.5 MB    |                                       |   0% \u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.1.31    | 159 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 87 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libexpat-2.6.3       | 69 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libzlib-1.2.13       | 56 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-3.12.2        | 13.9 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "python-3.12.2        | 13.9 MB   | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.1.31    | 159 KB    | #######4                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "openssl-3.4.1        | 2.5 MB    | 2                                     |   1% \u001b[A\n",
      "\n",
      "python-3.12.2        | 13.9 MB   | 3                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | #####3                                |  14% \u001b[A\u001b[A\u001b[A\n",
      "openssl-3.4.1        | 2.5 MB    | #8                                    |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.1.31    | 159 KB    | #############################8        |  81% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | ####7                                 |  13% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "certifi-2025.1.31    | 159 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "python-3.12.2        | 13.9 MB   | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | ############1                         |  33% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | ##########6                           |  29% \u001b[A\u001b[A\u001b[A\n",
      "python-3.12.2        | 13.9 MB   | #2                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 87 KB     | ######8                               |  18% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | #################9                    |  49% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | #######################3              |  63% \u001b[A\u001b[A\u001b[A\n",
      "openssl-3.4.1        | 2.5 MB    | #####6                                |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zlib-1.2.13          | 87 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-3.12.2        | 13.9 MB   | #6                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | #######################2              |  63% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | #############################3        |  79% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | ##############################6       |  83% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libexpat-2.6.3       | 69 KB     | ########5                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libexpat-2.6.3       | 69 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libsqlite-3.46.0     | 887 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libexpat-2.6.3       | 69 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | ####################################9 | 100% \u001b[A\u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "python-3.12.2        | 13.9 MB   | ##2                                   |   6% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.12      | 6 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.12      | 6 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libzlib-1.2.13       | 56 KB     | ##########5                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "openssl-3.4.1        | 2.5 MB    | #############3                        |  36% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python_abi-3.12      | 6 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libzlib-1.2.13       | 56 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libzlib-1.2.13       | 56 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "python-3.12.2        | 13.9 MB   | ###5                                  |  10% \u001b[A\n",
      "\n",
      "conda-25.3.0         | 1.1 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "python-3.12.2        | 13.9 MB   | ####1                                 |  11% \u001b[A\n",
      "python-3.12.2        | 13.9 MB   | ####5                                 |  12% \u001b[A\n",
      "python-3.12.2        | 13.9 MB   | #####                                 |  14% \u001b[A\n",
      "python-3.12.2        | 13.9 MB   | #####9                                |  16% \u001b[A\n",
      "python-3.12.2        | 13.9 MB   | ######6                               |  18% \u001b[A\n",
      "python-3.12.2        | 13.9 MB   | #######2                              |  20% \u001b[A\n",
      "python-3.12.2        | 13.9 MB   | ########5                             |  23% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy<2 in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'numpy<2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26\n",
      "  Downloading numpy-1.26.0-cp312-cp312-macosx_10_9_x86_64.whl.metadata (53 kB)\n",
      "Downloading numpy-1.26.0-cp312-cp312-macosx_10_9_x86_64.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy==1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4), y shape: (150,)\n",
      "X sample: [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "y sample: [0 0 0 0 0]\n",
      "------------- 모델 성능 평가 -------------\n",
      "최적 파라미터: {'C': 1, 'max_iter': 100, 'solver': 'lbfgs'}\n",
      "최고 교차검증 점수: 0.96\n",
      "테스트 데이터 정확도: 100.00%\n",
      "\n",
      "------------- 혼동 행렬 -------------\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "------------- 분류 보고서 -------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## 로지스틱 회귀 모델을 사용한 붓꽃 분류 코드 분석\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data = load_iris() # 붓꽃 데이터 로드\n",
    "X = data.data   # 특성(feature) 데이터\n",
    "y = data.target # 타겟(품종) 데이터\n",
    "\n",
    "# 붓꽃 데이터는 150개의 샘플로 구성되어 있으며, 각 샘플은 4개의 특성(꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)을 가집니다.\n",
    "# 타겟 데이터는 0, 1, 2의 세 가지 품종(0(setosa), 1(versicolor), 2(virginica)의 세 가지 품종)으로 구성되어 있습니다.\n",
    "print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "# print 함수를 사용하여 데이터의 일부를 출력합니다.\n",
    "print(f\"X sample: {X[:5]}\")\n",
    "print(f\"y sample: {y[:5]}\")\n",
    "\n",
    "# 2. 데이터 분할 (학습용 / 테스트용)\n",
    "# 전체 데이터를 훈련 세트(80%)와 테스트 세트(20%)로 분할합니다.\n",
    "# random_state=42로 설정하여 결과 재현성을 보장 (random_state는 임의의 값을 지정해주는 것으로, 같은 값을 넣으면 같은 결과를 얻을 수 있습니다.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. 특성 스케일링\n",
    "scaler = StandardScaler() # StandardScaler를 사용하여 각 특성의 평균을 0, 표준편차를 1로 변환\n",
    "# 로지스틱 회귀와 같은 선형 모델은 스케일링된 데이터에서 더 좋은 성능을 보입니다.\n",
    "X_train = scaler.fit_transform(X_train) # 훈련 데이터에서만 fit_transform을 사용하여 평균과 분산을 계산\n",
    "X_test = scaler.transform(X_test) # 테스트 데이터에는 계산된 값으로만 변환(transform)합니다.\n",
    "\n",
    "# 4. 모델 튜닝 (GridSearchCV): 로지스틱 회귀 모델 및 하이퍼파라미터 튜닝\n",
    "# LogisticRegression 모델을 초기화하고 GridSearchCV를 사용하여 최적 하이퍼파라미터를 탐색\n",
    "model = LogisticRegression()\n",
    "# C: 정규화 강도의 역수 (값이 작을수록 더 강한 정규화)\n",
    "# max_iter: 최적화 알고리즘의 최대 반복 횟수\n",
    "# solver: 최적화 알고리즘 ('lbfgs'와 'saga' 두 가지 비교)\n",
    "params = {'C': [0.1, 1, 10], 'max_iter': [100, 200, 300], 'solver': ['lbfgs', 'saga']}\n",
    "# cv=5로 5-겹 교차 검증을 수행합니다.\n",
    "grid = GridSearchCV(model, params, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 5. 최적 모델 선택: 모델 학습 (최적 파라미터 사용)\n",
    "# GridSearchCV가 찾은 최적 파라미터를 가진 모델을 선택합니다.\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# 6. 예측\n",
    "# 최적 모델을 사용하여 테스트 데이터에 대한 예측을 수행합니다.\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# 7. 모델 평가\n",
    "print(\"------------- 모델 성능 평가 -------------\")\n",
    "print(f\"최적 파라미터: {grid.best_params_}\")\n",
    "print(f\"최고 교차검증 점수: {grid.best_score_:.2f}\")\n",
    "print(f\"테스트 데이터 정확도: {accuracy_score(y_test, predictions) * 100:.2f}%\")\n",
    "\n",
    "# 8. 상세 보고서 출력\n",
    "print(\"\\n------------- 혼동 행렬 -------------\")\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(\"\\n------------- 분류 보고서 -------------\")\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "# print 함수를 사용하여 데이터의 일부를 출력합니다.\n",
    "print(f\"X sample: {X[:5]}\")\n",
    "print(f\"y sample: {y[:5]}\")\n",
    "\n",
    "X sample:\n",
    "[[5.1 3.5 1.4 0.2]\n",
    " [4.9 3.  1.4 0.2]\n",
    " [4.7 3.2 1.3 0.2]\n",
    " [4.6 3.1 1.5 0.2]\n",
    " [5.  3.6 1.4 0.2]]\n",
    "\n",
    "이것은 붓꽃(Iris) 데이터셋의 첫 5개 샘플에 대한 특성(feature) 데이터입니다. 각 행은 하나의 붓꽃 샘플을 나타내며, 각 열은 다음 4가지 특성 측정값을 순서대로 포함합니다:\n",
    "\n",
    "첫 번째 열: 꽃받침(Sepal) 길이 (cm)\n",
    "두 번째 열: 꽃받침(Sepal) 너비 (cm)\n",
    "세 번째 열: 꽃잎(Petal) 길이 (cm)\n",
    "네 번째 열: 꽃잎(Petal) 너비 (cm)\n",
    "\n",
    "예를 들어, 첫 번째 샘플은 꽃받침 길이 5.1cm, 꽃받침 너비 3.5cm, 꽃잎 길이 1.4cm, 꽃잎 너비 0.2cm를 가진 붓꽃입니다.\n",
    "\n",
    "y sample: [0 0 0 0 0]\n",
    "이것은 첫 5개 샘플의 타겟(target) 값으로, 각 붓꽃의 품종을 나타냅니다. Iris 데이터셋에서 타겟 값은 다음과 같이 매핑됩니다:\n",
    "\n",
    "0: Iris-setosa (세토사 품종)\n",
    "1: Iris-versicolor (버시컬러 품종)\n",
    "2: Iris-virginica (버지니카 품종)\n",
    "\n",
    "출력된 데이터를 보면, 첫 5개 샘플은 모두 값이 0이므로 모두 Iris-setosa 품종임을 알 수 있습니다.\n",
    "데이터 특성\n",
    "이 데이터를 통해 몇 가지 특징을 알 수 있습니다:\n",
    "\n",
    "Iris-setosa 품종은 꽃잎이 상대적으로 작습니다 (길이 약 1.3-1.5cm, 너비 0.2cm).\n",
    "꽃받침은 꽃잎보다 크며, 길이 4.6-5.1cm, 너비 3.0-3.6cm 범위에 있습니다.\n",
    "데이터셋의 처음 부분은 같은 품종(setosa)으로 구성되어 있습니다. 이는 데이터셋이 품종별로 정렬되어 있음을 시사합니다.\n",
    "\n",
    "이러한 특성 데이터를 바탕으로 로지스틱 회귀 모델이 각 품종을 구분하는 패턴을 학습합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 정확도: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split # 데이터를 학습용과 테스트용으로 나누는 함수\n",
    "from sklearn.datasets import load_iris # 아이리스 데이터셋을 불러오는 함수\n",
    "from sklearn.linear_model import LogisticRegression # 로지스틱 회귀 모델을 사용하는 클래스\n",
    "from sklearn.metrics import accuracy_score # 모델의 정확도를 측정하는 함수\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_iris()  # Iris 꽃 데이터셋 사용\n",
    "# 아이리스 데이터셋은 150개의 샘플과 4개의 특성(꽃받침 길이, 꽃받침 폭, 꽃잎 길이, 꽃잎 폭)으로 구성되어 있으며, 3개의 클래스를 가진 데이터입니다.\n",
    "X = data.data    # 입력 데이터\n",
    "y = data.target  # 출력 데이터(클래스 라벨)\n",
    "# X는 독립 변수(특성, 입력 데이터)이며, y는 종속 변수(클래스 레이블)입니다.\n",
    "\n",
    "# 데이터 분할 (학습용/테스트용)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# train_test_split 함수를 사용하여 데이터를 학습용(80%)과 테스트용(20%)으로 분할# random_state 파라미터는 결과의 재현성을 위해 난수 시드를 설정\n",
    "# 같은 값으로 지정하면 매 실행 시 동일한 분할\n",
    "\n",
    "# 로지스틱 회귀 모델 초기화 및 학습\n",
    "model = LogisticRegression(max_iter=200)  # 최대 반복 횟수를 지정 (기본값 100)\n",
    "model.fit(X_train, y_train)\n",
    "# LogisticRegression(max_iter=200)는 로지스틱 회귀 모델을 초기화\n",
    "# 반복 횟수를 200으로 설정하여 학습의 수렴 속도를 높입니다.\n",
    "# fit() 메서드를 통해 학습용 데이터를 모델에 학습시킵니다. 이 과정을 통해 모델은 데이터 패턴을 학습하고, 입력 특성이 주어졌을 때 특정 클래스를 예측하는 방법을 익힙니다.\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(X_test)\n",
    "# predict() 메서드를 통해 테스트용 데이터에 대한 예측 수행\n",
    "# 학습된 모델을 사용하여 X_test의 각 샘플이 어떤 클래스에 속하는지를 예측\n",
    "\n",
    "# 정확도 평가\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"모델 정확도: {accuracy * 100:.2f}%\")\n",
    "# accuracy_score() 함수를 사용하여 예측값과 실제값(y_test)을 비교하여 모델의 정확도를 계산합니다. 정확도는 올바르게 분류된 샘플의 비율을 나타냅니다.\n",
    "# 출력시, 정확도는 백분율 형식으로 포맷팅하여 표시합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 뉴럴 네트워크를 사용한 이진 분류 프로그램으로 PyTorch를 활용하여 10개의 특징(feature)를 가진 데이터를 학습하고, 이를 기반으로 데이터를 이진 분류(0 또는 1)하는 모델을 훈련하는 예제입니다.\n",
    "- 데이터를 생성 및 분리.\n",
    "- 딥러닝 모델 설계.\n",
    "- 모델 학습(Training) 및 테스트(Test).\n",
    "- 결과 출력(테스트 손실 및 정확도).\n",
    "학습 데이터는 가상의 데이터로 생성되며, 현실에서의 \"합격 여부\", \"환자 건강 상태(정상 또는 이상)\" 같은 이진 분류 문제를 모방한 예제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 0 | 손실: 0.7898\n",
      "에포크 10 | 손실: 0.7473\n",
      "에포크 20 | 손실: 0.7102\n",
      "에포크 30 | 손실: 0.6777\n",
      "에포크 40 | 손실: 0.6492\n",
      "에포크 50 | 손실: 0.6241\n",
      "에포크 60 | 손실: 0.6019\n",
      "에포크 70 | 손실: 0.5821\n",
      "에포크 80 | 손실: 0.5645\n",
      "에포크 90 | 손실: 0.5487\n",
      "테스트 정확도: 76.50%\n"
     ]
    }
   ],
   "source": [
    "# 위 코드는 PyTorch를 사용하여 간단한 신경망 모델을 구현하고, 이진 분류 문제를 해결하는 과정을 보여줍니다.\n",
    "# 단계별로 자세히 설명해 드리겠습니다.\n",
    "\n",
    "import torch  # 딥러닝 프레임워크\n",
    "import torch.nn as nn  # 신경망 설계를 위한 모듈\n",
    "import torch.optim as optim  # 최적화 함수 (파라미터 업데이트)\n",
    "from sklearn.datasets import make_classification  # 데이터를 생성하는 모듈\n",
    "from sklearn.model_selection import train_test_split  # 데이터 분리 도구\n",
    "\n",
    "# (1) 데이터 생성\n",
    "# make_classification 함수를 사용하여 인공적인 분류 데이터셋을 생성합니다.\n",
    "# 1000개의 샘플, 각 샘플당 10개의 특성을 가지는 이진 분류(2개 클래스) 데이터셋을 만듭니다.\n",
    "# 이진 분류를 수행하기 위해 2개의 클래스로 데이터 생성\n",
    "# random_state=42로 설정하여 결과 재현성을 보장합니다.\n",
    "# 결과 재현성이란, 같은 코드를 실행해도 항상 같은 결과가 나오도록 하는 것을 의미합니다.\n",
    "# 42로 설정한 이유는 딥러닝 분야에서 많이 사용하는 숫자이기 때문입니다.\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "\n",
    "# 데이터를 학습용(80%)과 테스트용(20%)으로 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# (2) 데이터를 PyTorch 텐서로 변환\n",
    "# PyTorch는 내부적으로 tensor 데이터 타입을 사용함\n",
    "# 학습용 데이터, 입력 데이터는 float32 타입으로 변환합니다.\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # 특징 데이터\n",
    "# 레이블 데이터 y를 (n, 1) 형태로 변환하여 손실 함수의 입력 형태에 맞게 조정합니다\n",
    "# view(-1, 1)는 배열을 2차원으로 만들며, -1은 자동으로 크기를 맞추라는 의미입니다.\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # 레이블 데이터 (-1, 1로 reshape)\n",
    "# 테스트용 데이터\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# (3) 신경망 모델 정의하기\n",
    "# 신경망 모델인 \"SimpleNN\" 클래스 정의\n",
    "# nn.Module을 상속받아 정의\n",
    "# 하나의 선형 레이어(nn.Linear)만 사용하는 매우 단순한 구조입니다.\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer = nn.Linear(10, 1)  # 선형 레이어. 입력층: 10개 노드 (특성 수), 출력층: 1개 노드 (이진 분류)\n",
    "\n",
    "    # 데이터가 네트워크를 통과하는 순전파 과정을 정의합니다.\n",
    "    # 선형 계산 후 시그모이드 활성화 함수를 적용하여 0과 1 사이의 확률값을 출력합니다.\n",
    "    # 이진 분류 문제에서는 시그모이드 함수를 사용하여 확률값을 계산합니다.\n",
    "    # 시그모이드 함수의 특징은 0과 1 사이의 값을 출력하며, 출력값을 확률로 해석할 수 있습니다.\n",
    "    def forward(self, x):\n",
    "        # 순전파(forward) 함수. 입력 데이터를 받아 결과를 반환\n",
    "        return torch.sigmoid(self.layer(x))\n",
    "\n",
    "# 모델 객체 생성\n",
    "model = SimpleNN()\n",
    "\n",
    "# (4) 손실 함수와 최적화 함수 정의\n",
    "\n",
    "# 손실 함수: BCELoss - 이진 분류(Binary Classification)를 위한 손실 함수\n",
    "criterion = nn.BCELoss() # 이진 분류 문제에 적합한 손실 함수, 예측 확률과 실제 레이블 간의 오차를 측정합니다.\n",
    "# 최적화 함수: SGD(Stochastic Gradient Descen, (확률적 경사 하강법)\n",
    "# 학습률(learning rate)은 0.01로 설정됩니다.\n",
    "# 모델의 모든 파라미터(가중치와 편향)에 대해 최적화를 수행합니다.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# (5) 모델 학습(Training using model.train())\n",
    "epochs = 100  # 데이터 전체를 100번 학습\n",
    "for epoch in range(epochs):  # 각 에포크(epoch) 반복\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    optimizer.zero_grad()  # 이전 에포크에서 계산된 기울기(gradient) 초기화\n",
    "    y_pred = model(X_train_tensor)  # 입력 데이터를 모델에 전달해 예측값 계산\n",
    "    loss = criterion(y_pred, y_train_tensor)  # 예측값과 실제값을 비교해 손실 계산\n",
    "    loss.backward()   # 손실에 대한 기울기(gradient)를 계산합니다(역전파).\n",
    "    optimizer.step()  # 계산된 기울기를 사용하여 모델 파라미터(가중치, 편향)를 업데이트합니다.\n",
    "\n",
    "    # 10 에포크마다 손실 값을 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"에포크 {epoch} | 손실: {loss.item():.4f}\")  # 현재 에포크와 손실 값 출력\n",
    "\n",
    "# (6) 모델 평가(Test using model.eval())\n",
    "model.eval()  # 모델을 평가 모드로 설정합니다(드롭아웃 등의 학습 전용 기능 비활성화).\n",
    "with torch.no_grad():  # 테스트 중에는 기울기 계산이 필요 없으므로 메모리 사용량을 줄이고 속도를 높입니다.\n",
    "    y_test_pred = model(X_test_tensor)  # 테스트 데이터를 사용해 예측값 계산\n",
    "    y_test_pred = (y_test_pred > 0.5).float()  # 모델 출력(확률)을 0.5를 기준으로 이진 클래스(0 또는 1)로 변환합니다.\n",
    "    # 정확도 계산\n",
    "    accuracy = (y_test_pred.eq(y_test_tensor).sum().item()) / y_test_tensor.shape[0]\n",
    "    print(f\"테스트 정확도: {accuracy * 100:.2f}%\")  # 테스트 정확도 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에포크별 손실(Loss) 변화\n",
    "\n",
    "```bash\n",
    "에포크 0 | 손실: 0.7898\n",
    "에포크 10 | 손실: 0.7473\n",
    "에포크 20 | 손실: 0.7102\n",
    "에포크 30 | 손실: 0.6777\n",
    "에포크 40 | 손실: 0.6492\n",
    "에포크 50 | 손실: 0.6241\n",
    "에포크 60 | 손실: 0.6019\n",
    "에포크 70 | 손실: 0.5821\n",
    "에포크 80 | 손실: 0.5645\n",
    "에포크 90 | 손실: 0.5487\n",
    "```\n",
    "\n",
    "손실 감소 추세 분석\n",
    "\n",
    "초기 손실: 학습을 시작한 에포크 0에서 손실값은 0.7898로 비교적 높습니다.<br>\n",
    "지속적인 감소: 학습이 진행됨에 따라 손실값이 꾸준히 감소하고 있습니다.<br>\n",
    "감소 속도: 초기에는 빠르게 감소하다가 학습이 진행될수록 감소 속도가 완만해집니다.<br>\n",
    "\n",
    "에포크 0-10: 약 0.0425 감소 (0.7898 → 0.7473)<br>\n",
    "에포크 80-90: 약 0.0158 감소 (0.5645 → 0.5487)<br>\n",
    "\n",
    "\n",
    "수렴 경향: 학습 후반부에도 여전히 손실이 감소하고 있어, 더 많은 에포크로 학습하면 손실이 더 낮아질 가능성이 있습니다.<br>\n",
    "\n",
    "최종 테스트 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `딥러닝(DL, Deep Learning)`\n",
    "  \n",
    "  > #### 정의 : ML 중에서 최적화된 기술, 인공신경망 기반 학습 기법\n",
    "  \n",
    "  - 핵심 구조: 다층 신경망(Neural Network)\n",
    "\n",
    "    - 대규모 데이터, 특징 추출, 높은 성능, 비정형 데이터 처리\n",
    "\n",
    "    - 딥러닝 응용 사례: 이미지 인식(ex. 고양이와 개 구분), 자연어 처리(ex. 번역 모델, 챗봇), 생성 모델(ex. GAN을 활용한 이미지 생성)\n",
    "\n",
    "    - 인공신경망은 각 뉴런의 입력 신호 x에 신호의 가중치(weight) w를 곱한 값으로 신호가 들어오며, 임계값으로 편향 b를 더하여 뉴런의 신호 출력 여부를 결정합니다. \n",
    "  \n",
    "    - 뉴런간의 전달된 신호들의 값을 모두 합하고 활성화 함수(activation function)를 통해 값이 0보다 크면 1, 0보다 작으면 0을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN(Deep Neural Network)의 기본 구조\n",
    "DNN은 크게 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성됩니다. \n",
    "\n",
    "이들 층은 다음과 같은 방식으로 연결됩니다:\n",
    "\n",
    "- 입력층 (Input Layer):\n",
    "\n",
    "  - 모델에 전달되는 데이터가 첫 번째 층으로 입력됩니다.\n",
    "  - 각 입력 노드는 데이터의 특성(feature)을 나타냅니다.\n",
    "\n",
    "- 은닉층 (Hidden Layer):\n",
    "\n",
    "  - 입력 데이터는 하나 이상의 은닉층을 통해 처리됩니다.\n",
    "\n",
    "  - 은닉층은 가중치(weights)와 편향(bias)을 이용해 입력 데이터를 변형하고, 활성화 함수를 통해 비선형성을 추가하여 더 복잡한 패턴을 학습합니다.\n",
    "\n",
    "  - DNN에서 중요한 점은 여러 개의 은닉층을 사용하여 데이터의 고차원적인 표현을 학습하는 것입니다.\n",
    "\n",
    "- 출력층 (Output Layer):\n",
    "\n",
    "  - 모델의 최종 예측값을 계산하는 층입니다.\n",
    "  - 분류 문제에서는 보통 소프트맥스(Softmax) 함수로 확률값을 출력하고, 회귀 문제에서는 선형(Linear) 출력을 제공합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 알고리즘\n",
    "  \n",
    "  a. 합성곱 신경망(Convolutional Neural Network, CNN)은 이미지 데이터 처리에 사용되는 신경망입니다.\n",
    "  \n",
    "  b. 순환 신경망(Recurrent Neural Network, RNN)은 순차 데이터(시계열 데이터) 처리를 위해 설계된 신경망입니다.\n",
    "  \n",
    "  c. 트랜스포머(Transformers)는 자연어 처리에서 순차적으로 처리하지 않고 모든 입력 데이터를 동시에 처리할 수 있도록 설계된 모델입니다.\n",
    "\n",
    "  d. 생성적 적대 신경망(Generative Adversarial Network,GAN)은 생성자와 판별자를 상호 적대적으로 학습시켜 데이터를 생성하는 모델입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/1.2_AI-ML-DL 계층적 구조.png\" width=\"400\" height=\"\" >\n",
    "<figcaption>그림 1.2 AI-ML-DL 계층적 구조</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 합성곱 연산은 필터를 가중치로 사용하고 입력데이터 전체에 반복적으로 적용하여 변수의 수를 줄입니다.\n",
    "\n",
    "필터를 적용하여 특징맵(feature map)을 추출하는 컨볼루션 레이어(convolution layer), 특징의 수를 줄이는 풀링 레이어(pooling layer), \n",
    "\n",
    "모든 노드들이 연결되어 있는 완전연결 레이어(fully connected layer)로 구성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/1.3_컨볼루션 연산.png\" width=\"800\" height=\"\" >\n",
    "<figcaption>그림 1.3 컨볼루션 연산</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > ### 호환성\n",
    "파이토치는 TensorFlow, Keras와 같은 다른 딥러닝 프레임워크와 비교하여 더 직관적이고 파이썬 친화적인 코드 작성이 가능합니다. 또한, PyTorch와 TensorFlow는 모델을 서로 변환할 수 있는 도구도 제공됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 신경망의 기본 작동 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망의 핵심 작동 원리(입력 → 가중치 → 출력 → 손실 함수 → 역전파) 학습.\n",
    "- 신경망 구조는 데이터를 입력하는 입력층(Input Layer), \n",
    "- 입력 데이터를 분석하고 패턴을 학습하는 은닉층(Hidden Layer), \n",
    "- 결과를 반환하는 출력층(Output Layer)으로 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/1.4_신경망 구조.png\" width=\"600\">\n",
    "<figcaption>그림 1.4 신경망 구조(은닉층 2개)</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터 → 가중치 계산 (y = wx + b).\n",
    "- 활성화 함수 (ex. Sigmoid, ReLU 등) 사용으로 비선형성을 추가.\n",
    "- 출력 결과를 손실 함수(Loss Function)로 평가 후 역전파 학습."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `코드 예제`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch를 통해 신경망의 기본 논리 구현 알아보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터와 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "inputs = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "targets = torch.tensor([[2.0], [4.0], [6.0], [8.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단순 선형 모델 정의 (y = Wx + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Linear(1, 1)  # 입력 1차원, 출력 1차원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실 함수와 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # PyTorch 라이브러리\n",
    "import torch.nn as nn  # 신경망 설계를 위한 모듈\n",
    "import torch.optim as optim  # 최적화를 위한 모듈\n",
    "\n",
    "# 100번 학습 반복(Epoch)\n",
    "for epoch in range(100):\n",
    "\n",
    "    # 1. 모델을 사용하여 입력값(inputs)에 대한 출력값(outputs)을 계산\n",
    "    #    즉, 현재 학습된 가중치와 편향을 바탕으로 예측을 수행\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # 2. 출력값(outputs)과 실제 정답(targets)을 비교해 손실(loss) 계산\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # 3. 기울기를 초기화 (이전 에포크의 기울기를 지움)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. 손실(loss)에 기반한 기울기를 계산\n",
    "    #    각 가중치와 편향(weight, bias)이 손실을 줄이기 위해 얼마나 조정되어야 하는지 저장\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. 계산된 기울기를 사용해 모델의 파라미터(가중치와 편향)를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "# 최종 학습이 완료된 후, 학습된 가중치(weight)와 편향(bias)를 출력\n",
    "print(f'학습된 가중치: {model.weight.item()}, 편향: {model.bias.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 코드는 단순한 선형 회귀 로직(y = Wx + b)을 구현한 것으로, 딥러닝 신경망의 핵심 작업을 간략히 보여주고 있습니다.\n",
    "- 모델이 \"데이터로부터 가중치(W)와 편향(b)을 학습\" 하는 과정입니다.\n",
    "- loss = criterion(outputs, targets)  # 손실 계산\n",
    "- loss.backward()  # 자동 미분\n",
    "- optimizer.step()  # 파라미터 업데이트\n",
    "\n",
    "※ 이 코드의 핵심은:\n",
    "\n",
    "- 모델이 학습: 데이터를 보고 손실을 최소화하기 위해 자신의 파라미터를 지속적으로 조정.\n",
    "- 가중치와 편향 업데이트: 손실 감소를 위해 학습된 파라미터 출력.\n",
    "\n",
    "※ 위 주석을 참고해 한 줄씩 분석하면서 학습하면 딥러닝 학습 과정의 기본 흐름을 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 데이터셋(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서 사용되는 데이터셋은 주로 문제의 유형에 따라 크게 구분할 수 있습니다. 주요 데이터셋의 구분은 다음과 같습니다:\n",
    "\n",
    "### 1. **이미지 데이터셋**\n",
    "   - **분류(Classification)**: 주어진 이미지를 여러 클래스 중 하나로 분류하는 문제입니다.\n",
    "     - 예시: \n",
    "       - **MNIST** (Modified National Institute of Standards and Technology): 손으로 쓴 숫자 이미지 (0-9)\n",
    "         - MNIST는 28x28 픽셀의 손글씨 숫자 이미지 약 70,000장을 포함하고 있으며, 이 중 약 60,000장은 훈련용, 10,000장은 테스트용으로 사용됩니다. 이 데이터셋은 기계 학습 및 딥러닝 연구의 표준 benchmark로 자리잡고 있습니다.\n",
    "       - **CIFAR-10 / CIFAR-100** (Canadian Institute For Advanced Research): 10개/100개 클래스의 색상 이미지\n",
    "         - **CIFAR-10** 구성: 10개의 클래스 (비행기, 자동차, 새, 고양이, 사슴, 개, 개구리, 말, 배, 무).\n",
    "           - 이미지 수: 총 60,000개의 32x32 픽셀 컬러 이미지. (훈련 세트: 50,000개 이미지 / 테스트 세트: 10,000개 이미지.)\n",
    "           - 특징: 비교적 단순한 분류 작업으로, 다양한 이미지 인식 알고리즘을 테스트하는 데 적합합니다.\n",
    "         - **CIFAR-100** 구성: 100개의 클래스. 각 클래스는 60개 이미지로 이루어져 있으며, 20개의 상위 레이블로 분류됩니다 (예: 자전거와 같은 항목들은 '탈 것'라는 상위 레이블에 속합니다).\n",
    "           - 이미지 수: 전체 60,000개의 32x32 픽셀 컬러 이미지.(훈련 세트: 50,000개 이미지 / 테스트 세트: 10,000개 이미지.)\n",
    "           - 특징: CIFAR-10보다 더 많은 클래스와 더 세분화된 분류 문제를 제공하여, 보다 복잡한 모델을 평가할 수 있습니다.\n",
    "       - **ImageNet**: 1000개 이상의 객체를 포함한 대형 이미지 데이터셋\n",
    "   - **객체 검출(Object Detection)**: 이미지에서 객체의 위치와 클래스를 예측하는 문제입니다.\n",
    "     - 예시:\n",
    "       - **COCO**: 객체 검출, 키포인트 인식, 이미지 캡셔닝 등 다양한 태스크를 지원하는 데이터셋\n",
    "   - **세그멘테이션(Segmentation)**: 이미지를 픽셀 단위로 분할하는 문제입니다.\n",
    "     - 예시:\n",
    "       - **PASCAL VOC**: 객체 검출 및 세그멘테이션을 포함한 이미지 데이터셋\n",
    "       - **Cityscapes**: 도시 환경에서의 픽셀 수준 이미지 분할 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/1.5_mnist_hand.png\" width=\"600\">\n",
    "<figcaption>그림 1.5 mnist_손글씨</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **텍스트 데이터셋**\n",
    "   - **자연어 처리(NLP)**: 텍스트를 처리하는 문제로, 텍스트 분류, 번역, 감정 분석, 질의 응답 등 다양한 태스크가 포함됩니다.\n",
    "  \n",
    "       - **IMDB**: 영화 리뷰 감정 분석 데이터셋\n",
    "       - **SQuAD**: 질문-답변 데이터셋\n",
    "       - **GLUE**: 여러 자연어 처리 태스크를 위한 벤치마크 데이터셋\n",
    "\n",
    "### 3. **시계열 데이터셋**\n",
    "   - **시계열 예측(Time Series Forecasting)**: 시간에 따른 데이터 변화를 예측하는 문제입니다.\n",
    "\n",
    "       - **Kaggle에서 제공하는 시계열 데이터셋**: 주식, 날씨 예측 등 다양한 시계열 데이터\n",
    "\n",
    "### 4. **추천 시스템(Recommendation Systems) 데이터셋**\n",
    "   - 사용자에게 아이템을 추천하는 문제입니다.\n",
    "       - **MovieLens**: 영화 추천 데이터셋\n",
    "       - **Netflix Prize Dataset**: 넷플릭스 영화 추천 데이터셋\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `딥러닝 예제 맛보기`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FashionMNIST 활용 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 데이터 준비\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 2. 모델 정의 (간단한 신경망)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        # 입력: 28x28 크기의 이미지 -> 128 차원\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        # 출력: 10개의 클래스 (패션 아이템 종류)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # 이미지를 1D 벡터로 변환\n",
    "        x = torch.relu(self.fc1(x))  # 활성화 함수 ReLU\n",
    "        x = self.fc2(x)              # 출력\n",
    "        return x\n",
    "\n",
    "# 3. 모델 초기화 및 설정\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저\n",
    "\n",
    "# 4. 모델 훈련\n",
    "def train(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0     # 에폭마다 손실 초기화\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()  # 기울기 초기화\n",
    "            output = model(data)   # 모델 예측\n",
    "            loss = criterion(output, target)  # 손실 계산\n",
    "            loss.backward()        # 역전파\n",
    "            optimizer.step()       # 가중치 업데이트\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "# 5. 모델 평가\n",
    "def test(model, test_loader):\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # 그래디언트 계산을 하지 않음\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            # 가장 높은 확률을 가진 클래스를 예측\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()  # 맞춘 개수 카운트\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# 훈련\n",
    "train(model, train_loader, criterion, optimizer, epochs=5)\n",
    "\n",
    "# 테스트\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시각화를 위한 함수 정의\n",
    "def visualize_predictions(model, test_loader, num_images=10):\n",
    "    \"\"\"\n",
    "    모델의 예측 결과를 시각화하고, 예측 라벨과 실제 라벨을 비교합니다.\n",
    "\n",
    "    :param model: 학습된 PyTorch 모델\n",
    "    :param test_loader: 테스트 데이터 로더\n",
    "    :param num_images: 시각화할 이미지의 수\n",
    "    \"\"\"\n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "\n",
    "    # 테스트 데이터에서 배치 하나 가져오기\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    # 모델로 예측 수행\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)  # 가장 높은 확률을 가진 클래스 선택\n",
    "\n",
    "    # FashionMNIST 클래스 이름 정의\n",
    "    classes = [\n",
    "        'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "    ]\n",
    "\n",
    "    # 시각화\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for idx in range(num_images):\n",
    "        ax = plt.subplot(2, num_images // 2, idx + 1)\n",
    "        ax.imshow(images[idx].numpy().squeeze(), cmap=\"gray\")  # 이미지를 Grayscale 형태로 표시\n",
    "        ax.set_title(f\"Pred: {classes[predicted[idx]]}\\nTrue: {classes[labels[idx]]}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 시각화 함수 호출\n",
    "visualize_predictions(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
