{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8XHHT99-Slm"
   },
   "source": [
    "### Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순전파 및 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaV6bKLx-Sls"
   },
   "source": [
    "> ## 학습 목표\n",
    "-   **순전파의 개념 이해** : 신경망에서 순전파가 어떻게 이루어지는지 설명하고, 입력 데이터가 출력으로 변환되는 과정을 단계별로 이해할 수 있다.\n",
    "    \n",
    "-   **역전파의 원리 이해** : 역전파(backpropagation)의 기본 원리와 기울기 계산 과정을 이해하고, 손실 함수의 기울기를 통해 네트워크 파라미터를 업데이트하는 방법을 설명할 수 있다.\n",
    "    \n",
    "-   **순전파와 역전파 연결 이해** : 순전파와 역전파가 신경망 학습 과정에서 어떻게 상호작용하는지를 이해하고, 두 과정의 역할을 명확히 구분할 수 있다.\n",
    "    \n",
    "-   **파이토치에서의 구현** : 파이토치를 사용하여 순전파와 역전파 과정이 포함된 신경망 모델을 구현하고, 이를 통해 학습 과정을 시뮬레이션할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`순전파(Forward Propagation) & 역전파(Backward Propagation)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ 순전파(Forward Propagation)\n",
    "\n",
    "순전파는 **입력 데이터를 신경망의 각 계층을 거쳐 출력 값으로 변환**하는 과정입니다.\n",
    "\n",
    "1.  **목적**:\n",
    "    \n",
    "    -   모델이 입력 데이터를 받아 예측 결과를 생성하는 과정입니다.\n",
    "    -   손실(loss)을 계산하기 위해 예측 값을 생성합니다.\n",
    "2.  **작동 방식**:\n",
    "    \n",
    "    -   입력 데이터가 첫 번째 계층(입력 계층)에서 시작해 네트워크의 각 계층(은닉 계층 및 출력 계층)을 통과합니다.\n",
    "    -   각 계층에서 `가중치(weight)와 편향(bias)`이 적용되고, 활성화 함수(activation function)가 적용되어 출력값을 계산합니다.\n",
    "    -   마지막 출력 계층에서 결과값(예: 분류 확률)이 도출됩니다.\n",
    "\n",
    "3.  **결과**:\n",
    "    \n",
    "    -   네트워크의 최종 출력 값과 실제 값 간의 오차(손실)를 계산합니다.\n",
    "    -   손실 함수(loss function)가 사용됩니다(예: MSE, Cross-Entropy 등).\n",
    "\n",
    "\n",
    "### ■ 역전파(Backward Propagation)\n",
    "\n",
    "역전파는 **손실을 기반으로 네트워크의 가중치와 편향을 업데이트하기 위해 기울기(gradient)를 계산하는 과정**입니다.\n",
    "\n",
    "1.  **목적**:\n",
    "    \n",
    "    -   손실 함수의 값을 최소화하기 위해 가중치와 편향을 조정합니다.\n",
    "    -   네트워크가 학습하도록 합니다.\n",
    "\n",
    "2.  **작동 방식**:\n",
    "    \n",
    "    -   손실 함수로부터 시작해 네트워크의 마지막 계층에서 첫 번째 계층(입력 계층) 방향으로 거꾸로 진행됩니다.\n",
    "    -   각 계층에서 `기울기(gradient)`를 계산하여 가중치와 편향의 변화량(업데이트 방향)을 결정합니다.\n",
    "    -   체인 룰(chain rule)을 사용해 각 계층의 기울기를 계산합니다.\n",
    "\n",
    "3.  **가중치 업데이트**:\n",
    "    \n",
    "    -   경사하강법(Gradient Descent) 등을 사용해 가중치를 업데이트합니다.\n",
    "\n",
    "4.  **결과**:\n",
    "    \n",
    "    -   네트워크의 가중치와 편향이 업데이트되며, 모델의 성능이 점진적으로 개선됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 순전파(Forward Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 데이터가 네트워크에 들어가면 각 층에서 순차적으로 연산이 이루어집니다.\n",
    "- 각 뉴런은 입력값에 가중치를 곱하고 그 결과를 활성화 함수에 통과시켜 출력을 생성합니다.\n",
    "- 이 과정은 네트워크의 각 층을 차례로 지나면서 최종 출력값을 얻는 과정입니다.\n",
    "- 파이토치에서 순전파는 forward() 메서드를 사용하여 구현됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **순전파 단계**\n",
    "  - 입력층 : 네트워크에 입력 데이터가 들어옵니다.\n",
    "  - 은닉층 : 입력 데이터는 가중치(weight)와 편향(bias)을 곱한 후 활성화 함수(activation function)를 거쳐 은닉층의 출력으로 변환됩니다.\n",
    "  - 출력층 : 은닉층의 출력값은 다시 가중치와 편향을 거쳐 최종 출력값을 생성합니다.\n",
    "  - 손실 계산 : 네트워크의 출력값과 실제 정답값을 비교하여 손실(loss)을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순전파 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. 신경망 모델 정의\n",
    "class SimpleNN(nn.Module):   # nn.Module을 상속하여 신경망 모델을 정의\n",
    "    def __init__(self):      # __init__ 는 신경망의 레이어를 정의\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 입력층 1, 은닉층 10, 출력층 1\n",
    "        self.fc1 = nn.Linear(1, 10)  # 입력층에서 은닉층\n",
    "        self.fc2 = nn.Linear(10, 1)  # 은닉층에서 출력층\n",
    "    # 순전파 구현\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 층에 대해 ReLU 활성화 함수 적용\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # 두 번째 층 (출력층)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. 모델 초기화\n",
    "model = SimpleNN()\n",
    "\n",
    "# 3. 임의의 입력 데이터 생성 (배치 크기 3, 입력 크기 1)\n",
    "input_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "\n",
    "# 4. 순전파 실행\n",
    "# model(input_data)를 호출하여 입력 데이터를 모델에 통과시켜 출력값을 계산\n",
    "output_data = model(input_data)\n",
    "\n",
    "# 5. 출력 확인\n",
    "print(\"입력 데이터:\\n\", input_data)\n",
    "print(\"모델 출력:\\n\", output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 역전파(Backward Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역전파는 순전파 후 계산된 손실을 기반으로 가중치와 편향을 업데이트하는 과정입니다. \n",
    "- **경사 하강법(Gradient Descent)** 을 이용하여 수행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역전파 단계\n",
    "- 출력층에서 오차 계산: 출력층에서 네트워크의 예측값과 실제값 간의 차이를 계산합니다. 손실함수를 통해 오차를 구합니다.\n",
    "- 오차의 미분값 계산: 오차를 이용해 가중치를 조정할 수 있도록 오차를 역으로 전파합니다.\n",
    "  - 출력층에서부터 각 층으로 오차의 기울기를 계산합니다. \n",
    "  - 체인 룰(Chain Rule)을 사용하여 각 층의 가중치가 오차에 미친 영향을 계산합니다.\n",
    "  - 체인 룰은 복합 함수의 미분을 계산하는 방법으로, 신경망에서 각 층의 미분값을 곱하여 오차를 전파하는 데 사용됩니다.\n",
    "- 가중치와 편향 업데이트: 계산된 기울기를 사용하여 가중치와 편향을 업데이트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 역전파 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2개의 클래스를 가진 이진 분류 문제 \n",
    "- 손실함수는 CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ` 적용 실습` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비 X: 입력, y: 목표 출력\n",
    "X = torch.tensor([[0.0, 0.0],\n",
    "                  [0.0, 1.0],\n",
    "                  [1.0, 0.0],\n",
    "                  [1.0, 1.0]], dtype=torch.float32)\n",
    "\n",
    "y = torch.tensor([0, 1, 1, 0], dtype=torch.long)  # XOR 문제 (레이블 0과 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 클래스 정의\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)  # 입력층 → 은닉층 (2개 뉴런)\n",
    "        self.fc2 = nn.Linear(2, 2)  # 은닉층 → 출력층 (2개 클래스)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))  # 은닉층에서 시그모이드 활성화 함수\n",
    "        x = self.fc2(x)  # 출력층은 CrossEntropyLoss 사용\n",
    "        return x\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수: CrossEntropyLoss (소프트맥스 포함)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저: 경사 하강법 (SGD)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 학습 과정 (순전파, 역전파, 가중치 업데이트)\n",
    "epochs = 10000\n",
    "for epoch in range(epochs): # 1000번 에폭 반복\n",
    "    # 순전파: 입력 데이터를 모델에 통과시켜 예측값을 얻음\n",
    "    output = model(X)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(output, y)\n",
    "\n",
    "    # 역전파\n",
    "    optimizer.zero_grad()  # 기울기 초기화\n",
    "    loss.backward()  # 기울기 계산\n",
    "    optimizer.step()  # 가중치 업데이트\n",
    "\n",
    "    # 1000번마다 손실 출력\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델에서 최종 예측\n",
    "with torch.no_grad(): # 기울기 계산 비활성화\n",
    "    output = model(X)\n",
    "    predicted = torch.argmax(output, dim=1) # 샘플에 대해 가장 높은 확률을 가진 클래스 예측\n",
    "    print(\"last prediction:\", predicted.numpy())  # x에 대한 레이블 y값 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. 신경망 정의 (SimpleNN)**\n",
    "\n",
    "  - fc1은 첫 번째 선형층(입력층 → 은닉층)입니다.\n",
    "  - fc2는 두 번째 선형층(은닉층 → 출력층)입니다.\n",
    "  - forward 함수는 순전파를 정의합니다. 입력을 은닉층에 통과시키고, 활성화 함수로 시그모이드를 적용한 후 출력층으로 전달합니다.\n",
    "\n",
    "**2. 모델 초기화** : 모델을 인스턴스화하고, 손실 함수와 최적화 방법을 설정합니다.\n",
    "\n",
    "**3. 훈련 과정**\n",
    "\n",
    "  - outputs = model(inputs)로 순전파가 이루어집니다.\n",
    "  - loss = criterion(outputs, targets)로 출력값과 실제 값 간의 손실을 계산합니다.\n",
    "  - loss.backward()로 역전파를 통해 기울기를 계산하고, optimizer.step()으로 가중치를 업데이트합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `데이터셋 load_breast_cancer 이용한 모델 만들기`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load_breast_cancer 데이터셋은 scikit-learn 라이브러리에서 제공하는 유방암(Breast Cancer) 진단에 관한 데이터셋입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Breast Cancer Wisconsin (Diagnostic) Dataset**은 유방암 종양의 양성과 악성을 진단하는 데 사용되는 데이터셋으로, 1995년에 공개되었습니다. \n",
    " \n",
    "이 데이터셋은 사이킷런(sklearn)의 데이터셋 모듈에서도 제공되며, 머신러닝 연구 및 교육 목적으로 널리 사용됩니다.\n",
    "\n",
    "\n",
    "### 1\\. **데이터셋 소개**\n",
    "\n",
    "-   **출처**: UCI Machine Learning Repository\n",
    "-   **목적**: 유방암 종양의 진단(양성 또는 악성) 예측\n",
    "-   **크기**:\n",
    "    -   **샘플 수**: 569개\n",
    "    -   **특성 수**: 30개 (수치형)\n",
    "    -   **클래스 수**: 2개 (양성 `B: Benign`, 악성 `M: Malignant`)\n",
    "\n",
    "\n",
    "### 2\\. **특성 정보**\n",
    "\n",
    "데이터셋은 유방암 종양 세포의 핵 특징을 기반으로 합니다. 30개의 수치형 특성은 다음과 같은 10가지 측정 지표의 평균값, 표준편차, 최댓값을 포함합니다:\n",
    "\n",
    "1.  **Radius**: 핵의 반지름(거리 중심-경계)\n",
    "2.  **Texture**: 그레이스케일의 표준편차\n",
    "3.  **Perimeter**: 경계 길이\n",
    "4.  **Area**: 핵 면적\n",
    "5.  **Smoothness**: 경계의 매끄러움\n",
    "6.  **Compactness**: 주변 면적 대비 둘레^2\n",
    "7.  **Concavity**: 경계의 오목한 부분의 정도\n",
    "8.  **Concave points**: 경계의 오목한 지점 수\n",
    "9.  **Symmetry**: 대칭성\n",
    "10.  **Fractal dimension**: 경계의 복잡성\n",
    "\n",
    "각 지표별로:\n",
    "\n",
    "-   `_mean`: 평균값\n",
    "-   `_se`: 표준 오차\n",
    "-   `_worst`: 최댓값  \n",
    "    이렇게 3가지의 값이 제공됩니다. 따라서 총 10 × 3 = 30개의 특성이 생성됩니다.\n",
    "\n",
    "\n",
    "### 3\\. **클래스 정보**\n",
    "\n",
    "-   **M (Malignant)**: 악성 (212개 샘플, 약 37%)\n",
    "-   **B (Benign)**: 양성 (357개 샘플, 약 63%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터셋 로딩\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**사이킷런에서의 데이터셋**\n",
    "\n",
    "- 사이킷런에서 load_breast_cancer를 사용하여 쉽게 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성과 레이블\n",
    "X = data.data  # 입력 특성\n",
    "y = data.target  # 타겟 (0: 악성, 1: 양성)\n",
    "\n",
    "# 특성 이름과 타겟 이름\n",
    "print(data.feature_names)\n",
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터프레임 생성\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 크기와 기본 통계\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n",
    "# 데이터 개요\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 분포 확인\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 클래스 분포 시각화\n",
    "df['target'].value_counts().plot(kind='bar', color=['blue', 'RED'])\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class (0: Malignant, 1: Benign)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 몇 개만 확인\n",
    "sns.pairplot(df[['target'] + list(df.columns[:5])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 데이터 전처리 (훈련/테스트 데이터 분리 및 정규화)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 표준화: StandardScaler를 사용하여 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 데이터를 파이토치 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 4. 신경망 모델 정의\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(30, 64)  # 30개의 입력 특성\n",
    "        self.fc2 = nn.Linear(64, 32)  # 중간층\n",
    "        self.fc3 = nn.Linear(32, 1)   # 출력층\n",
    "        self.sigmoid = nn.Sigmoid()   # 이진 분류를 위한 시그모이드 활성화 함수\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # 첫 번째 층: ReLU 활성화\n",
    "        x = torch.relu(self.fc2(x))  # 두 번째 층: ReLU 활성화\n",
    "        x = self.fc3(x)              # 출력층\n",
    "        x = self.sigmoid(x)          # 시그모이드 활성화\n",
    "        return x\n",
    "\n",
    "# 5. 모델 초기화\n",
    "model = SimpleNN()\n",
    "criterion = nn.BCELoss()  # 이진 분류 문제이므로 BCELoss 사용\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 6. 학습\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # 순전파\n",
    "    output = model(X_train_tensor)\n",
    "\n",
    "    # 손실 계산\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "\n",
    "    # 역전파\n",
    "    optimizer.zero_grad()  # 기울기 초기화\n",
    "    loss.backward()        # 기울기 계산\n",
    "    optimizer.step()       # 파라미터 업데이트\n",
    "\n",
    "    # 10번마다 손실 값 출력\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 7. 테스트\n",
    "with torch.no_grad():\n",
    "    # 모델 예측\n",
    "    output = model(X_test_tensor)\n",
    "    predicted = (output > 0.5).float()  # 0.5를 기준으로 이진 분류\n",
    "\n",
    "    # 정확도 계산\n",
    "    accuracy = accuracy_score(y_test, predicted.numpy())\n",
    "    print(f'Accuracy on test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 모델 훈련\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 모델 생성 및 학습\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트를 사용한 특성 중요도 시각화:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 특성 중요도 추출\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), data.feature_names[indices], rotation=68)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **데이터셋의 활용**\n",
    "- 머신러닝 및 딥러닝 기법의 학습과 실험\n",
    "- 분류 문제 연구 및 알고리즘 비교\n",
    "- 특성 선택 및 중요도 분석\n",
    "\n",
    "\n",
    "※ Breast Cancer Wisconsin (Diagnostic) Dataset은 크기가 작아 초급자와 전문가 모두에게 적합하며, 다양한 머신러닝 알고리즘의 성능 비교에 자주 사용됩니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
